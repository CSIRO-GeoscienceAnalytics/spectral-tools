<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.1" />
<title>spex.extraction.specex API documentation</title>
<meta name="description" content="The class SpectralExtraction is used for finding spectral features in a spectral dataset. Additionally, the user can also
perform a search of the â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS_CHTML'></script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>spex.extraction.specex</code></h1>
</header>
<section id="section-intro">
<p>The class SpectralExtraction is used for finding spectral features in a spectral dataset. Additionally, the user can also
perform a search of the returned features via the feature_search convenience functions. The user can however just search
the returned features themselves if they wish.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
The class SpectralExtraction is used for finding spectral features in a spectral dataset. Additionally, the user can also
perform a search of the returned features via the feature_search convenience functions. The user can however just search
the returned features themselves if they wish.
&#34;&#34;&#34;
import numpy as np
import numpy.polynomial.polynomial as rolypoly
from scipy.signal import find_peaks
from scipy import interpolate
import multiprocessing as mp
from spex.ext.chulls import get_absorption
import hdbscan
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
from spex.io.instruments import Tsg, CsvSpectra, NumpySpectra, ImageData
import warnings
warnings.simplefilter(&#39;ignore&#39;, np.RankWarning) # stop polyfit rankwarnings

def _find_indices(values, ordinates):
    &#34;&#34;&#34;
    Returns the closest indices of the values being sought in the ordinates array
    Args:
        values ():A list of ordinate values that you want the indexes of
        ordinates ():A numpy array of the ordinates to be searched

    Returns: A list of the indices in ordinates that are closest to the values

    could also just do this with np.searchsorted if the ordinates are in ascending order
    &#34;&#34;&#34;
    indices = []
    for val in values:
        indices.append(np.argmin(np.abs(ordinates - val)))
    return indices


def _generator(spectral_array):
    &#34;&#34;&#34;
    a generator for a numpy data array
    The assumption is that the last axis is the actual data while the other leading axis are spatial or sample
    e.g. data_array of shape [N, M] has N sample with M spectral bands, a data array of shape [N, P, M] has N rows by
    P columns with M spectral bands

    If the incoming array is one dimensional it will simply return that spectrum
    :return: an iterator over the spectra
    &#34;&#34;&#34;
    ndims = spectral_array.ndim
    if ndims == 1:
        yield spectral_array
    elif ndims == 2:
        for spectrum in spectral_array:
            yield spectrum
    elif ndims == 3:
        for row in spectral_array:
            for spectrum in row:
                yield spectrum
    else:
        return -99


def _mp_process_data(spectral_array, ordinates, feature_proximity, max_features, prominence):
    &#34;&#34;&#34;
    Used for multiprocessing of image files (since they are usually large). Allows for large speed ups in processing

    Args:
        spectral_array (iterable): the spectral data
        ordinates (numpy): the spectral ordinates associated with the spectral data
        feature_proximity (int): how many bands apart at a minimum should features be
        max_features (int): the maximum number of features to return
        prominence (float): a cutoff value for the feature depth (below which its assumed its not a feature)

    Returns: the calculated feature information

    &#34;&#34;&#34;
    feature_info = []
    for row in spectral_array:
        for spectrum in row:
            # return the peaks_ordinates, prominences and widths
            feature_info.append(
                process_single_signal(spectrum, ordinates, distance=feature_proximity, max_features=max_features,
                                      prominence=prominence))
    return feature_info


def process_single_signal(signal, ordinates, distance=None, prominence=None, max_features=4, old_school=False):
    &#34;&#34;&#34;
    Get the peaks for a single spectrum

    Args:
        old_school (bool): if True use the location of the minimum and 1 channel to the left and right to use
            a 2nd order polynomial to solve for the wavelength location. if False use a 6 order polynomial.

        signal (ndarray): the signal to be processed (usually a numpy array)

        ordinates (ndarray): the ordinates corresponding to the signal (usually a numpy array)

        distance (int, optional): Required minimal horizontal distance (&gt;= 1) in samples between neighbouring peaks.
            Smaller peaks are removed first until the condition is fulfilled for all remaining peaks.

        prominence (number or ndarray or sequence, optional): Required prominence of peaks. Either a number, ``None``,
            an array matching `x` or a 2-element sequence of the former. The first element is always interpreted as the
            minimal and the second, if supplied, as the maximal required prominence.

        max_features (int): maximum number of features to report back

    Returns:
        ndarray: A numpy array of size (9 x number of requested features) values representing the feature parameters for each of
            the found features

    Comments:
        The 9 parameters associated with a single feature are as follows,

        0: feature wavelength

        1: feature depth (given as prominence in the find_peaks routine). These can be considered as relative depths.
        See the explanation of prominence in
        https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html

        2: feature width (FWHM)

        3: feature asymmetry. A number between -1 and 1. -1 is heavily left symmetrical, 0 is symmetrical and 1 is
        heavily right symmetrical

        4: feature peak heights. These are different than the feature depths. The feature peak heights are the height
        from the base line to the top of the peak

        5: Wavelength location of the left shoulder of a feature (from a prominence point of view)

        6: Wavelength location of the right shoulder of a feature (from a prominence point of view)

        7: Wavelength location of the left hand side of the FWHM

        8: Wavelength location of the right hand side of the FWHM


    &#34;&#34;&#34;
    # # get the spectrum after taking care of the hull
    # get the indicies for the peaks. If the width isn&#39;t set to 0 (or some other value) it wont return peaks_properties
    peaks, peaks_properties = find_peaks(signal, distance=distance, prominence=prominence, width=0.0, height=0.0)
    # sort from deepest to smallest
    indices = np.flip(np.argsort(peaks_properties[&#39;prominences&#39;]))

    # get rid of additional unwanted features
    if len(indices) &gt;= max_features:
        indices = indices[:max_features]

    # reorder the data from deepest to smallest
    peaks = peaks[indices]
    for value in peaks_properties:
        peaks_properties[value] = peaks_properties[value][indices]

    # calculate the wavelength positions in the ordinate space with a 2nd order polynomial e.g quadratic
    # this is done because we assume the actual minima may be between the signal indices
    peak_ordinates = []
    if old_school:
        for t in peaks:
            if 0 &lt; t &lt; len(signal) - 2:
                x = ordinates[t - 1:t + 2]
                y = signal[t - 1:t + 2]
                coefficients = np.polyfit(x, y, 2)
                peak_ordinates.append(-coefficients[1] / (2 * coefficients[0]))
            else:
                peak_ordinates.append(ordinates[t])
    else:
        for index, t in enumerate(peaks):
            if 10 &lt; t &lt; len(signal) - 11: # end point limits
                order = 2
                if peaks_properties[&#39;widths&#39;][index] &lt; 2:
                    start = t-1
                    stop = t+2
                else:
                    start = t-10
                    stop = t + 10
                    order = 6
                newx = ordinates[start:stop]
                newy = signal[start:stop]
                coefficients, _ = rolypoly.polyfit(newx, newy, order, full=True)
                tempy = rolypoly.polyval(newx, coefficients)
                max_loc = np.argmax(tempy)
                if order &gt; 2:
                    if np.logical_or(max_loc == 0, max_loc == newx.size-1):
                        max_loc = 10
                    coefficients, _ = rolypoly.polyfit(newx[max_loc - 1:max_loc + 2], tempy[max_loc - 1:max_loc + 2], 2, full=True)
                answer = -coefficients[1] / (2 * coefficients[2])
                peak_ordinates.append(answer)
            else:
                peak_ordinates.append(ordinates[t])

    # pad out the results if needed to suit the users number of features request
    if np.size(peak_ordinates) &lt; max_features:
        peak_ordinates = np.append(peak_ordinates, np.zeros(max_features - np.size(peak_ordinates)))
        for value in peaks_properties:
            peaks_properties[value] = np.append(peaks_properties[value],
                                                np.zeros(max_features - np.size(peaks_properties[value])))

    # Interpolate the left and right locations of the FWHM to wavelength space
    left_ips = np.interp(peaks_properties[&#39;left_ips&#39;], np.arange(ordinates.shape[0]), ordinates)
    right_ips = np.interp(peaks_properties[&#39;right_ips&#39;], np.arange(ordinates.shape[0]), ordinates)
    widths = right_ips - left_ips
    left_bases = np.interp(peaks_properties[&#39;left_bases&#39;], np.arange(ordinates.shape[0]), ordinates)
    right_bases = np.interp(peaks_properties[&#39;right_bases&#39;], np.arange(ordinates.shape[0]), ordinates)
    asymmetry = np.zeros(left_ips.shape)
    indx = (right_ips - left_ips) &gt; 0
    asymmetry[indx] = 2 * ((right_ips[indx] - np.array(peak_ordinates)[indx]) / (right_ips[indx] - left_ips[indx]) - 0.5)
    return_values = np.asarray(peak_ordinates), peaks_properties[&#39;prominences&#39;], widths, asymmetry, \
                    peaks_properties[&#39;peak_heights&#39;], left_bases, right_bases, left_ips, right_ips

    return return_values


class SpectralExtraction:
    &#34;&#34;&#34;
    A class for extracting spectral feature information from a spectral dataset. The dataset is represented by an
    instance of the Instrument class e.g. from `spex.io.instruments`

    The features that are returned (either peaks or absorptions depending on how you set it up) are not just
    randomly selected between your ordinate_inspection_range they are ordered. Meaning if you said give me 4
    features then it is the 4 deepest features in descending order.

    The actual calculation of the location of the peaks is done via the find_peaks routine in scikit-learn. To get
    actual peak locations in a wavelength format they are further inferred from the fitting of a 3 point
    quadratic (since the band where the deepest point occurs might not technically be the actual location of the
    deepest feature) and solving ``dy/dx = 0 = -b^2/2a``

    find_peaks: https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html

    I would recommend having a read through the various routines and returns from fit_peaks so you can get to grips
    with what it actually being returned from this class.

    It is up to the user to select the appropriate processing required in terms of hull corrections via the
    ``do_hull`` keyword and/or invert keyword.

    Warnings:
        YOU CAN DO SOME REALLY WEIRD STUFF IF YOU DON&#39;T PAY ATTENTION

        If you are chasing the spectral absorption features.If the incoming data is spectral reflectance then set
        ``do_hull=True`` and ``hull_type`` to your preferred. If the data is already in the appropriate format i.e. hull removed
        etc, then this isn&#39;t needed and do_hull=False should be set.

        If the incoming spectra are TIR reflectance spectra then you set ``do_hull=False`` and ``hull_type=3``
        A bit weird but it tells the class that what you actually want is a baseline correction. If you don&#39;t want a
        baseline correction then just leave hull_type as the default. As an aside, the hull type actually applied with
        those settings is a hull removed type so it does not alter the heights of the peaks during the baseline
        correction.

        When you do the above with TIR spectra you are actually finding the locations of the peaks!!!
        Not the reflectance absorptions. If we assume ``1=R+E+T`` (reflectance + emissivity + transmittance) then this is
        finding the locations of the emissivity absorptions (assuming T is zero).

    If you want to use a different hull type for TIR then do the following, set ``invert=True`` (does a 1-spectra),
    set ``do_hull=True`` (this acts as a baseline correction) and set ``hull_type`` to your preference.

    If you want the reflectance absorptions (emissivity peaks) of your TIR spectra then set ``do_hull=True``,
    ``invert=False`` and ``hull_type=0`` (hull quotient) or ``1`` (hull removed). There is no baseline correction here though.

    So lots of weirdness can ensue if you don&#39;t pay attention to the keywords and your spectra.

    Attributes:
        dimension_shape (tuple): The shape of the spectral data set

        dimensions (int): how many dimensions are in the spectral dataset

        do_hull (bool): using a hull correction prior to feature extraction

        feature_info (ndarray): the extracted feature information

        feature_proximity (int): closest allowable feature distance

        feature_search_indices (ndarray): A boolean array of the same sample size as the spectral data where the
            feature_search_space criteria was met.

        feature_search_parameters (ndarray): The result of running a feature search over the extracted spectral features

        feature_search_space (list): A list of tuples designating the search criteria of the feature_info

        hull_type: The hull type used in the feature extraction

        instrument (obj): An instance of the `spex.io.instruments` class

        invert (bool): Should the spectral data be inverted prior to feature extraction

        max_features (int): How many spectral features to find

        ordinates (ndarray): the ordinates of the spectral data

        ordinates_inspection_range (list): The spectral range over which to perform feature extraction

        prominence_depth (float): minimum acceptable prominence depth

        range_index (list): the indices of the ordinates array that correspond to the ordinates_inspection_range

    &#34;&#34;&#34;

    def __init__(self, instrument_files, max_features=4, feature_proximity=None, prominence_depth=None,
                 ordinate_inspection_range=None, do_hull=False, hull_type=0, invert=False, order=0, nrows=None):
        &#34;&#34;&#34;
        Args:
            instrument_files (list): A list that depending on the contents will be used to open the spectral data. The
                can have the following form [csv path+filename], or [wavelengths(ndarray), spectra (ndarray)], or
                [envi spectral path+filename, envi spectral hdr path+filename], or [tsg spectral path+filename, tsg *.tsg path+filename]

            max_features (int, optional): The maximum number of features to report back. Defaults to 4

            feature_proximity (int, optional): The closest distance between features to be considered as a spectral
                feature. Defaults to None. This is an index value so a value of 3 says I wont accept any absorptions that are
                closer than 3 bands to the left and right of my feature (dominant feature always wins)

            prominence_depth (float, optional): The minimum feature depth to be considered. Defaults to None

            ordinate_inspection_range (list, optional): A start and stop bounding range for the feature search. Defaults to
                the instruments spectral range e.g. [start, stop] -&gt; [2000., 2500.]

            do_hull (bool, optional): Perform hull operations on the incoming spectral data. Defaults to False

            hull_type (int, optional): 0: hull quotient, 1: hull removed 3: baseline correction, Defaults to 0

            invert (bool, optional): should the spectra be inverted (e.g. 1-spectra) before doing any processing, Defaults to False

        &#34;&#34;&#34;
        if isinstance(instrument_files, Tsg):
            self.instrument = instrument_files
        else:
            io_length = len(instrument_files)
            is_numpy = isinstance(instrument_files[0], np.ndarray)
            if io_length &gt; 1:
                envi_or_not = instrument_files[1].__contains__(&#34;.hdr&#34;)
                if is_numpy:
                    self.instrument = NumpySpectra(instrument_files)
                else:
                    if envi_or_not:
                        self.instrument = ImageData(instrument_files)
                    else:
                        self.instrument = Tsg(instrument_files)
            else:
                self.instrument = CsvSpectra(instrument_files, order=order, nrows=nrows)

        self.dimensions = self.instrument.dimensions
        self.dimension_shape = self.instrument.dimension_shape
        self.do_hull = do_hull
        self.hull_type = hull_type
        self.invert = invert
        self.ordinates = self.instrument.get_ordinates()
        # self.metadata = instrument.metadata
        self._generator = self.instrument.datagenerator()

        if ordinate_inspection_range is None:
            self.ordinates_inspection_range = [self.ordinates[0], self.ordinates[-1]]
            self.range_index = _find_indices([self.ordinates[0], self.ordinates[-1]], self.ordinates)
        else:
            self.ordinates_inspection_range = ordinate_inspection_range
            self.range_index = _find_indices(ordinate_inspection_range, self.ordinates)

        # feature extraction parameters
        self.max_features = max_features
        self.feature_proximity = feature_proximity
        self.prominence_depth = prominence_depth

        # the extracted feature info populates this variable
        self.feature_info = None
        # self.average_feature_info = None

        # feature searching data
        self.feature_search_indices = None
        self.feature_search_space = None
        self.feature_search_parameters = None

        # clustering parameters
        self.scalar = None
        self.clusterer = None
        self.features_to_include = [1, 0, 0, 0, 0, 0, 0, 0, 0]
        self.number_of_features = 2
        self.feature = 0
        self.min_cluster_size = 5
        self.min_samples = None
        self.cluster_selection_epsilon = 0.0
        self.cluster_selection_method = &#39;eom&#39;
        self.allow_single_cluster = False
        self.prediction_data = False

    def process_data(self):
        &#34;&#34;&#34;
        Process the spectral data according to the class instantiation variables and return the feature results

        Returns:
            An array of values for each found feature (all zeros if no feature is found). The array has dimensions
            of (N x 9 x number of features requested), where is N are the number of spectral samples. The features are given
            from the largest to the smallest so entry features[0,:,0] is for the first feature, feature[0,:,1] is for the
            second and so on.

        Comments:
            The 9 parameters associated with a single feature are as follows:

            0: feature wavelength

            1: feature depth (given as prominence in the find_peaks routine). These can be considered as relative depths.
            See the explanation of prominence in
            https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html

            2: feature width (FWHM)

            3: feature asymmetry. A number between -1 and 1. -1 is heavily left symmetrical, 0 is symmetrical and 1 is
            heavily right symmetrical

            4: feature peak heights. These are different than the feature depths. The feature peak heights are the height
            from the base line to the top of the peak

            5: Wavelength location of the left shoulder of a feature (from a prominence point of view)

            6: Wavelength location of the right shoulder of a feature (from a prominence point of view)

            7: Wavelength location of the left hand side of the FWHM

            8: Wavelength location of the right hand side of the FWHM

        &#34;&#34;&#34;
        self.feature_info = np.squeeze(self._process_spectral_array())
        return self.feature_info

    def get_features(self):
        &#34;&#34;&#34;
        Return the extracted feature information

        Returns:
            An array of values for each found feature (all zeros if no feature is found). The array has dimensions
            of (N x 9 x number of features requested) or (N x M x 9 x number of features requested), where the first is
            for N samples and the second is for an NxM image. The features are given from the largest to the smallest
            so that entry features[0,:,0] is for the first feature, feature[0,:,1] is for the
            second and so on.

        Comments:
            The 9 parameters associated with a single feature are as follows:

            0: feature wavelength

            1: feature depth (given as prominence in the find_peaks routine). These can be considered as relative depths.
            See the explanation of prominence in
            https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html

            2: feature width (FWHM)

            3: feature asymmetry. A number between -1 and 1. -1 is heavily left symmetrical, 0 is symmetrical and 1 is
            heavily right symmetrical

            4: feature peak heights. These are different than the feature depths. The feature peak heights are the height
            from the base line to the top of the peak

            5: Wavelength location of the left shoulder of a feature (from a prominence point of view)

            6: Wavelength location of the right shoulder of a feature (from a prominence point of view)

            7: Wavelength location of the left hand side of the FWHM

            8: Wavelength location of the right hand side of the FWHM

        &#34;&#34;&#34;
        return self.feature_info

    def _feature_search(self, search_space, exclude=False):
        &#34;&#34;&#34;
        Search N feature_info entries in the provided search space

        Args:
            search_space (list): A list comprising the search space

        Returns:
            same as feature_search

        &#34;&#34;&#34;
        feature_info = self.feature_info

        s_space = np.array(search_space)
        starts = s_space[::2]
        stops = s_space[1::2]
        diffs = stops - starts
        search = np.ones(feature_info[..., 0, :].shape).astype(bool)
        for index, val in enumerate(diffs):
            if val &gt; 0:
                temp = (feature_info[..., index, :] &gt; starts[index]) &amp; (feature_info[..., index, :] &lt; stops[index])
                search = search &amp; temp

        if exclude:
            search = np.max(search, axis=-1)

        return search

    @staticmethod
    def find_first_occurence(x):
        y = np.zeros_like(x, dtype=bool)
        idx = np.arange(len(x)), x.argmax(axis=1)
        y[idx] = x[idx]
        return y

    @staticmethod
    def static_feature_search(search_space, exclude=False, feature_info=None, single=False):
        &#34;&#34;&#34;
        Search N feature_info entries in the provided search space

        Args:
            search_space (list): A list comprising the search space

        Returns:
            same as feature_search

        &#34;&#34;&#34;
        if feature_info is None:
            return -99

        s_space = np.array(search_space)
        starts = s_space[::2]
        stops = s_space[1::2]
        diffs = stops - starts
        search = np.ones(feature_info[..., 0, :].shape).astype(bool)
        for index, val in enumerate(diffs):
            if val &gt; 0:
                temp = (feature_info[..., index, :] &gt; starts[index]) &amp; (feature_info[..., index, :] &lt; stops[index])
                search = search &amp; temp

        if exclude:
            search = np.max(search, axis=-1)
        if single:
            search = SpectralExtraction.find_first_occurence(search)

        return search

    def _single_spectrum_search(self, search_space):
        &#34;&#34;&#34;
        Search the single spectrum feature_info

        Args:
            search_space (list): list of tuples comprising the search space

        Returns:
            A boolean array of size 1 indicating if it satisfied the search criteria, an array of feature parameters
            corresponding to those samples that meet the search criteria.
            -99 if nothing found

        &#34;&#34;&#34;
        feature_info = self.feature_info
        combos_keep = []
        # (wavelength, wavelength radius, depth minimum, width, width radius, use asymmetry, asymmetry gt or lt 0)
        for combos in search_space:
            if len(combos) == 7:
                tester = np.isclose(feature_info[0, :], np.abs(combos[0]), atol=combos[1])
                if combos[0] &lt; 0:
                    tester = np.logical_not(tester)
                else:
                    # see if the user cares about the wavelength
                    if combos[0] != 0:
                        wave_test = np.isclose(feature_info[0, :], np.abs(combos[0]), atol=combos[1])
                    else:
                        # here we are saying that the wavelength is not of interest
                        wave_test = True
                    depth_test = feature_info[1, :] &gt; combos[2]
                    tester = np.logical_and(tester, depth_test)
                    if combos[3] &gt; 0:
                        widths = np.isclose(feature_info[2, :], combos[3], atol=combos[4])
                        tester = np.logical_and(tester, widths)
                    if combos[5] != 0:
                        if combos[5] &lt; 0:
                            asymmetry = feature_info[3, :] &lt; combos[6]
                        else:
                            asymmetry = feature_info[3, :] &gt; combos[6]
                        tester = np.logical_and(tester, asymmetry)
                    tester = np.logical_and(tester, wave_test)

                # append this combination to the results
                combos_keep.append(self._first_appearance_only(tester))
                # the first_appearance_only is so we don&#39;t include multiple results from one sample e.g. a deepest
                # wavelength and a 10th deepest
        else:
            print(
                &#39;The feature space combinations are not of length 7 (wavelength, wavelength radius, depth minimum, &#39;
                &#39;width, width radius, asymmetry left or right, lt or gt) &#39;)

        # keep only those entries for which all combo searches are true
        all_true = np.all(np.sum(np.asarray(combos_keep)).astype(bool))

        # return an boolean array with the same spatial size as the input spectral array
        self.feature_search_indices = all_true

        combo_feature_parameters = []
        for index, var in enumerate(combos_keep):
            if search_space[index][0] &gt;= 0:
                what_feature = var
                combo_feature_parameters.append(feature_info[:, what_feature])
                self.feature_search_parameters = np.transpose(np.asarray(combo_feature_parameters), (1, 0))

        if self.feature_search_parameters.size == 0:
            self.feature_search_parameters = -99
            self.feature_search_indices = -99

        return self.feature_search_indices, self.feature_search_parameters

    def _profile_spectrum_search(self, search_space):
        &#34;&#34;&#34;
        Search N feature_info entries in the provided search space

        Args:
            search_space (list): list of tuples comprising the search space

        Returns:
            same as feature_search

        &#34;&#34;&#34;
        feature_info = self.feature_info
        combos_keep = []

        # # build the search
        #         # q = (feature_info[:, 1, :] &gt; 0.2) &amp; (feature_info[:, 0, :] &gt; 800) &amp; (feature_info[:, 0, :] &lt; 1000) &amp; (
        #         #             feature_info[:, 2, :] &gt; 100) &amp; (feature_info[:, 2, :] &lt; 400)
        #         # # identify which samples matched the search
        #         # qq = np.where(np.max(
        #         #     (feature_info[:, 1, :] &gt; 0.2) &amp; (feature_info[:, 0, :] &gt; 800) &amp; (feature_info[:, 0, :] &lt; 1000) &amp; (
        #         #                 feature_info[:, 2, :] &gt; 100) &amp; (feature_info[:, 2, :] &lt; 400), axis=1))[0]
        #         # # get the feature that matched the search
        #         # feat = np.argmax(q[qq, :], axis=1)
        # #  get the actual features
        # bob = feature_info[qq, :, feat]
        # (wavelength, wavelength radius, depth minimum, width, width radius)
        # a = (feature_info[:, 0, :] &gt; 750) &amp; (feature_info[:, 0, :] &lt; 1000)
        # b = (feature_info[:, 1, :] &gt; 0.0) &amp; (feature_info[:, 1, :] &lt; 0.5)
        # c = (feature_info[:, 2, :] &gt; 100) &amp; (feature_info[:, 2, :] &lt; 400)
        # d = (feature_info[:, 3, :] &gt; 0) &amp; (feature_info[:, 3, :] &lt; 1)
        # m = np.max(a &amp; b &amp; c &amp; d, axis=-1)
        # am = np.argmax(a &amp; b &amp; c &amp; d, axis=-1)

        for combos in search_space:
            if len(combos) == 7:
                if combos[0] &lt; 0:
                    wave_test = np.isclose(feature_info[:, 0, :], np.abs(combos[0]), atol=combos[1])
                    null_samples = np.where(wave_test)
                    wave_test[null_samples[0], :] = True
                    wave_test = np.logical_not(wave_test)
                    combos_keep.append(wave_test)
                else:
                    # see if the user cares about the wavelength
                    if combos[0] != 0:
                        wave_test = np.isclose(feature_info[:, 0, :], np.abs(combos[0]), atol=combos[1])
                    else:
                        # here we are saying that the wavelength is not of interest
                        wave_test = np.ones((feature_info.shape[0], feature_info.shape[2]), dtype=bool)
                    depth_test = feature_info[:, 1, :] &gt;= combos[2]
                    if combos[3] &gt; 0:
                        widths = np.isclose(feature_info[:, 2, :], combos[3], atol=combos[4])
                        tester = np.logical_and(depth_test, widths)
                    else:
                        tester = depth_test
                    if combos[5] != 0:
                        if combos[5] &lt; 0:
                            asymmetry = feature_info[:, 3, :] &lt; combos[6]
                        else:
                            asymmetry = feature_info[:, 3, :] &gt; combos[6]
                        tester = np.logical_and(tester, asymmetry)
                    tester = np.logical_and(tester, wave_test)

                    # append this combination to the results
                    combos_keep.append(self._first_appearance_only(tester))
                    # the first_appearance_only is so we don&#39;t include multiple results from one sample e.g. a deepest
                    # wavelength and a 10th deepest
            else:
                print(
                    &#39;The feature space combinations are not of length 5 (wavelength, wavelength radius, &#39;
                    &#39;depth minimum, width, width radius) &#39;)

        # keep only those entries for which all combo searches are true
        all_true = np.all(np.sum(np.asarray(combos_keep), axis=2).astype(bool), axis=0)

        # return an boolean array with the same spatial size as the input spectral array
        self.feature_search_indices = all_true

        # okay this works (this is for a 2D spatial image e.g. corescan)
        all_true_indices = np.where(all_true)
        combo_feature_parameters = []
        for index, var in enumerate(combos_keep):
            if search_space[index][0] &gt;= 0:
                what_feature = np.where(var[all_true_indices[0], :])[1]
                combo_feature_parameters.append(feature_info[all_true_indices[0], :, what_feature])
                self.feature_search_parameters = np.transpose(np.asarray(combo_feature_parameters), (1, 2, 0))

        if self.feature_search_parameters.size == 0:
            self.feature_search_parameters = -99
        if self.feature_search_indices.size == 0:
            self.feature_search_indices = -99

        return self.feature_search_indices, self.feature_search_parameters

    def _imager_spectral_search(self, search_space):
        &#34;&#34;&#34;
        Search the image returned feature_info in the provided search space

        Args:
            search_space (list): list of tuples comprising the search space

        Returns:
            same as feature_Search

        &#34;&#34;&#34;

        feature_info = self.feature_info
        combos_keep = []
        # (wavelength, wavelength radius, depth minimum, width, width radius)
        for combos in search_space:
            if len(combos) == 7:
                if combos[0] &lt; 0:
                    wave_test = np.isclose(feature_info[:, :, 0, :], np.abs(combos[0]), atol=combos[1])
                    null_samples = np.where(wave_test)
                    wave_test[null_samples[0], null_samples[1], :] = True
                    wave_test = np.logical_not(wave_test)
                    combos_keep.append(wave_test)
                else:
                    # see if the user cares about the wavelength
                    if combos[0] != 0:
                        wave_test = np.isclose(feature_info[:, :, 0, :], np.abs(combos[0]), atol=combos[1])
                    else:
                        # here we are saying that the wavelength is not of interest
                        wave_test = np.ones((feature_info.shape[0], feature_info.shape[1], feature_info.shape[3]),
                                            dtype=bool)
                    depth_test = feature_info[:, :, 1, :] &gt;= combos[2]
                    if combos[3] &gt; 0:
                        widths = np.isclose(feature_info[:, :, 2, :], combos[3], atol=combos[4])
                        tester = np.logical_and(depth_test, widths)
                    else:
                        tester = depth_test
                    if combos[5] != 0:
                        if combos[5] &lt; 0:
                            asymmetry = feature_info[:, :, 3, :] &lt; combos[6]
                        else:
                            asymmetry = feature_info[:, :, 3, :] &gt; combos[6]
                        tester = np.logical_and(tester, asymmetry)

                    tester = np.logical_and(tester, wave_test)
                    # append this combination to the results
                    combos_keep.append(self._first_appearance_only(tester))
                    # the first_appearance_only is so we dont inlcude multiple results from one sample e.g. a deepest
                    # wavelength and a 10th deepest
            else:
                print(
                    &#39;The feature space combinations are not of length 7 (wavelength, wavelength radius, &#39;
                    &#39;depth minimum, width, width radius) &#39;)

        # keep only those entries for which all combo searches are true
        all_true = np.all(np.sum(np.asarray(combos_keep), axis=3).astype(bool), axis=0)

        # return an boolean array with the same spatial size as the input spectral array
        self.feature_search_indices = all_true

        # okay this works (this is for a 2D spatial image e.g. corescan)
        all_true_indices = np.where(all_true)
        combo_feature_parameters = []
        for index, var in enumerate(combos_keep):
            if search_space[index][0] &gt;= 0:
                what_feature = np.where(var[all_true_indices[0], all_true_indices[1], :])[1]
                combo_feature_parameters.append(
                    feature_info[all_true_indices[0], all_true_indices[1], :, what_feature])
            self.feature_search_parameters = np.transpose(np.asarray(combo_feature_parameters), (1, 2, 0))

        if self.feature_search_parameters.size == 0:
            self.feature_search_parameters = -99
        if self.feature_search_indices.size == 0:
            self.feature_search_indices = -99

        return self.feature_search_indices, self.feature_search_parameters

    def feature_search(self, search_space=[0, 0, 0, 0, 0, 0, 0, 0], exclude=False):
        &#34;&#34;&#34;
        Return a boolean array of locations where the search space criteria is satisfied and an array of parameters
        that correspond to the feature information for those samples who meet the search criteria

        Args:
            search_space (list): A list of tuples that define the search space parameters [(search 1), (search 2),...(search N)]

        A single_tuple consists of: (wavelength, wavelength search radius, minimum depth, width, width search radius,
         asymmetry direction, asymmetry &lt; or &gt;)

        Returns:
            feature_search_indices, feature_search_parameters (or -99 if nothing found)

             feature_search_indices (ndarray): A boolean array of the same sample size as the spectral data where the
                feature_search_space criteria was met.

            feature_search_parameters (ndarray): The result of running a feature search over the extracted spectral features

        Comments:
            In a given search tuple a value of 0 for the wavelength, or width generally  means I don&#39;t care about this
            parameter. So if you had a search tuple that looked like this (0,0,0.1,0,0,0,0) you would be saying I am
            only interested in all results where the minimum depth is greater than 0.1

            If you want to do a search and exclude a wavelength i.e. find all results where returned features do NOT
            contain this wavelength then set the wavelength to a negative value e.g. (-2210,..)
            NOTE: You should put your wavelength exclusions at the end of the search space list. The reason why is that
            even though a search space may have an exclusion in it the return is only comprised of the positive search&#39;s
            (see below for an example). So if you stick an exclusion in the middle of your entire search space, which
            you can do if you want, you need to keep track yourself of what search space the return values pertain to.

            If you want to search on asymmetry then set the 6th search parameter to a number greater than 0 for right
            symmetric or less than 0 for left symmetric. Set the last parameter to find those values either greater than
            or less than (dependant on left or right symmetric)

            So for example a search_space of,
            [(2160, 10, 0, 0, 0, 0, 0), (2210, 20, 0, 30, 5, 0, 0), (-2250, 20, 0, 0, 0, 0, 0)]
            will look for all samples where a feature is found to have feature wavelengths between (2160+/-10) AND
            (2210+/-20 with feature widths of 30+/-5) AND (no 2250+/-20 feature).
            The feature information returned though is only for the positive search components.

            If we use the example above and assume the features were initially extracted from an array
            of 5000 samples of which 120 matched the search criteria then it would return the following:

            boolean array: (5000), A numpy array: (120, 9, 2) NOTE: Only 2 search space criteria returned
            Any wavelengths set to negative (as in exclude those) are not included in the return result.

            Referencing the second array returned at (:,0,0) would give the wavelengths found for the first
            search_space (2160+/-10), (:,1,0) would give the depths and (:,2,0) would give the width and so on.
            (:,0,1) would give the wavelengths found for the second search_space (2210+/-20), (:,1,1)
             would give the depths, and (:,2,1) would give the widths (30+/-5) and so on.

        &#34;&#34;&#34;
        if self.feature_info is None:
            print(&#39;You need to process the data first: .process_data()&#39;)
            return

        ndims = self.dimensions + 1
        if len(search_space) == 8:
            return self._feature_search(search_space, exclude=exclude)

        # # todo get rid of the range check. if folks want to search outside of their range then let them.
        # # see if any of the search space values are outside of the spectral feature extraction ordinate inspection range
        # # if they are then they will produce a False simply by not being within the feature extraction inspection range
        # # To avoid this we clip the search range. It might not be the best approach but...
        # temp_search_space = []
        # for val in search_space:
        #     if abs(val[0]) &gt; 0:
        #         test_search = np.logical_and(abs(val[0]) &gt;= self.ordinates_inspection_range[0],
        #                                      abs(val[0]) &lt;= self.ordinates_inspection_range[1])
        #         if test_search:
        #             temp_search_space.append(val)
        #     else:
        #         temp_search_space.append(val)
        #
        # self.feature_search_space = search_space
        #
        # # (wavelength start, wavelength stop, depth start, depth stop, width start, width stop,
        # # asymetry start, asymetry stop)
        # for combos in search_space:
        #     if len(combos) == 7:
        #         if ndims == 2:
        #             return self._single_spectrum_search(search_space)
        #         elif ndims == 3:
        #             return self._profile_spectrum_search(search_space)
        #         elif ndims == 4:
        #             return self._imager_spectral_search(search_space)
        #         else:
        #             return -99, -99
        #     else:
        #         print(
        #             &#39;The feature space combinations are not of length 7 (wavelength, wavelength radius, &#39;
        #             &#39;depth minimum, width, width radius) &#39;)
        #         return -99

    @staticmethod
    def _first_appearance_only(x):
        &#34;&#34;&#34;
        Sets all occurrences of True in the last dimension to False with the exception of the first occurrence. Its used
        to ensure we don&#39;t keep duplicate values. In hindsight this can probably be done with np.unique I suspect or
        something like that. I&#39;ll look into that at a later date

        Args:
            x (): A numpy array of type bool eith 1D, 2D or 3D

        Returns:
            A numpy array of type bool

        &#34;&#34;&#34;
        y = np.zeros_like(x, dtype=bool)
        if x.ndim == 1:
            y = x.argmax()
        if x.ndim == 2:
            idx = np.arange(len(x)), x.argmax(axis=1)
            y[idx] = x[idx]
        elif x.ndim == 3:
            for index, var in enumerate(x):
                idx = np.arange(len(var)), var.argmax(axis=1)
                y[index, idx[0], idx[1]] = var[idx[0], idx[1]]
        return y

    def feature_data_to_new_range(self, new_range):
        &#34;&#34;&#34;
        Trims the returned feature info down to a new spectral range and returns it
        Does not overwrite the original data but it also does not store it.

        Args:
            new_range (list): a list of start and stop wavelength e.g. [2100, 2400]

        Returns:
            the extracted spectral feature information between the new_range

        &#34;&#34;&#34;
        max_number = 0
        indices = []
        if self.feature_info is None:
            self.process_data()

        # new_features_array = np.zeros(self.feature_info.shape)
        #
        # if self.dimensions == 2:
        #     pass
        # elif self.dimensions == 3:
        #     locations = np.where(np.logical_and(self.feature_info[:, :, 0, :] &gt;= new_range[0],
        #                             self.feature_info[:, :, 0, :] &lt;= new_range[1]))
        #     how_big = locations[0].size
        #     if how_big &gt; 0:
        #         new_features_array[locations[0], locations[1], :, :] = \
        #             self.feature_info[locations[0], locations[1], :, :]

        feature_data = self.feature_info
        for val in np.arange(feature_data.shape[0]):
            locations = np.where(np.logical_and(feature_data[val, 0, :] &gt;= new_range[0],
                                                feature_data[val, 0, :] &lt;= new_range[1]))[0]
            how_big = locations.size

            if how_big &gt; 0:
                indices.append((val, locations))
            if how_big &gt; max_number:
                max_number = how_big

        new_features_array = np.zeros((feature_data.shape[0], feature_data.shape[1], max_number))
        for val in indices:
            new_features_array[val[0], :, :val[1].size] = feature_data[val[0], :, :][:, val[1]]

        if new_features_array.size == 0:
            new_features_array = -99

        return new_features_array

    def get_feature_parameters(self):
        &#34;&#34;&#34;
        returns the results of a feature_search

        Returns:
            feature_search_indices, feature_search_parameters (or -99 if nothing found)

            feature_search_indices (ndarray): A boolean array of the same sample size as the spectral data where the
                feature_search_space criteria was met.

            feature_search_parameters (ndarray): The result of running a feature search over the extracted spectral features

        &#34;&#34;&#34;
        return self.feature_search_indices, self.feature_search_parameters

    def _process_spectral_array(self):
        &#34;&#34;&#34;
        This is an internal method so we can work with spectral image data.

        Returns:
            feature_info[N, M, 9] where N is the number of rows and M is the number of columns

        Comments:
            The 9 parameters associated with a single feature are as follows:

            0: feature wavelength

            1: feature depth (given as prominence in the find_peaks routine). These can be considered as relative depths.
            See the explanation of prominence in
            https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html

            2: feature width (FWHM)

            3:Feature asymmetry. A number between -1 and 1. -1 is heavily left symmetrical, 0 is symmetrical and 1 is
            heavily right symmetrical

            4: feature peak heights. These are different than the feature depths. The feature peak heights are the height
            from the base line to the top of the peak

            5: Wavelength location of the left shoulder of a feature (from a prominence point of view)

            6: Wavelength location of the right shoulder of a feature (from a prominence point of view)

            7: Wavelength location of the left hand side of the FWHM

            8: Wavelength location of the right hand side of the FWHM

        &#34;&#34;&#34;
        ordinate_range = self.ordinates_inspection_range
        ordinates = self.ordinates
        do_hull = self.do_hull
        hull_type = self.hull_type
        invert = self.invert
        feature_proximity = self.feature_proximity
        max_features = self.max_features
        prominence = self.prominence_depth
        spectral_array = self.instrument.return_all_spectra()
        ndims = self.dimensions

        feature_info = []
        # subset the data on wavelength if its called for
        if ordinate_range is not None:
            range_index = _find_indices(ordinate_range, ordinates)
            ordinates = ordinates[range_index[0]:range_index[1] + 1]
            if ndims == 1:
                spectral_array = spectral_array[range_index[0]:range_index[1] + 1]
            elif ndims == 2:
                spectral_array = spectral_array[:, range_index[0]:range_index[1] + 1]
            elif ndims == 3:
                spectral_array = spectral_array[:, :, range_index[0]:range_index[1] + 1]
            else:
                return -99
        spectral_array[spectral_array &lt; 0] = 0

        if invert:
            spectral_array = 1.0 - spectral_array

        # run a hull process if required
        if do_hull:
            spectral_array = get_absorption(ordinates, spectral_array, hull_type=hull_type)
        if hull_type == 3:
            # do a baseline correction. Should really only do this for spectral data
            spectral_array = get_absorption(ordinates, 1.0 - spectral_array, hull_type=1)

        if ndims != 3:
            for index, signal in enumerate(_generator(spectral_array)):
                # return the peaks_ordinates, prominences and widths
                feature_info.append(
                    process_single_signal(signal, ordinates, distance=feature_proximity, max_features=max_features,
                                          prominence=prominence))
        else:  # multiprocessing
            # for value in spectral_array:
            #     for signal in _generator(value):
            #         # return the peaks_ordinates, prominences and widths
            #         feature_info.append(
            #             process_single_signal(signal, ordinates, distance=feature_proximity, max_features=max_features,
            #                                   prominence=prominence))
            # The bit above is not how i wanted to do this but for some reasons unknown to me the multiprocessing
            # has now decided to be a massive pain in the arse and stop working. Ah well long processing times for
            # images it is then
            # Nope found the problem. To use multiprocessing on windows the routine has to be in a __main__ guard

            chunks = mp.cpu_count()
            rows = int(spectral_array.shape[0] / chunks)
            sub_arrays = np.split(spectral_array, [rows * val for val in np.arange(1, chunks)])
            inputs = []
            for entry in sub_arrays:
                inputs.append((entry, ordinates, self.feature_proximity, self.max_features, self.prominence_depth))
            pool = mp.Pool(chunks)
            pool_output = pool.starmap(_mp_process_data, inputs)  # Returns a list of lists
            pool.close()
            pool.join()
            feature_info = [entry for val in pool_output for entry in val]

        # store the data
        if spectral_array.ndim == 3:
            feature_info = np.reshape(np.asarray(feature_info),
                                      (spectral_array.shape[0], spectral_array.shape[1], 9, max_features))
        else:
            feature_info = np.asarray(feature_info)

        return feature_info

    def cluster_features(self, features_to_include=[1, 0, 0, 0, 0, 0, 0, 0, 0], number_of_features=2, feature=0,
                         min_cluster_size=5, min_samples=None, cluster_selection_epsilon=0.0,
                         cluster_selection_method=&#39;eom&#39;, allow_single_cluster=False, prediction_data=False):
        &#34;&#34;&#34;
        A method for clustering of the extracted features  as returned by `SpectralExtraction.process_data`.

        Additional:
            https://hdbscan.readthedocs.io/en/latest/index.html

            https://towardsdatascience.com/a-gentle-introduction-to-hdbscan-and-density-based-clustering-5fd79329c1e8

            https://pberba.github.io/stats/2020/01/17/hdbscan/

        Args:
            feature (int): which feature to use for clustering. This only applies if `number_of_features=1`

            prediction_data (bool): Whether to generate extra cached data for predicting labels or
                membership vectors few new unseen points later. If you wish to
                persist the clustering object for later re-use you probably want
                to set this to True. Defaults to False

            number_of_features (int): How many features to use for the clustering. Goes from deepest to smallest e.g. if
                `number_of_features = 2` then it will use the 1st and 2nd deepest features.

            features_to_include (list): A list of length 9 consisting of 1&#39;s and 0&#39;s used to switch a given feature on or
                off

            min_cluster_size (int): The minimum size of clusters; single linkage splits that contain
                fewer points than this will be considered points &#34;falling out&#34; of a
                cluster rather than a cluster splitting into two new clusters.

            min_samples (int): The number of samples in a neighbourhood for a point to be considered a core point.

            cluster_selection_epsilon (float): A distance threshold. Clusters below this value will be merged.

            cluster_selection_method (str): The method used to select clusters from the condensed tree. The
                standard approach for HDBSCAN* is to use an Excess of Mass algorithm
                to find the most persistent clusters. Alternatively you can instead
                select the clusters at the leaves of the tree -- this provides the
                most fine grained and homogeneous clusters. Options are: ``eom`` or ``leaf``

            allow_single_cluster (bool): By default HDBSCAN* will not produce a single cluster, setting this
                to True will override this and allow single cluster results in
                the case that you feel this is a valid result for your dataset.

        Returns:
            -99: when the clustering can not be completed (a message is printed)

            class_labels (ndarry): An array of the same dimensions (non spectral) as the input data containing the
                resultant class labels as returned by `HDBSCAN`. The input data are the extracted spectral features.

            class_probabilities (ndarray): An array of the same dimensions (non spectral) as the input data containing
                the cluster probabilities as returned by `HDBSCAN`. The input data are the extracted spectral features.

        &#34;&#34;&#34;
        # first check if any feature info exists
        if self.feature_info is None:
            print(&#34;You need to do a feature extraction first using process_data()&#34;)
            self.process_data()

        # write some information to the state variables
        self.features_to_include = features_to_include
        self.number_of_features = number_of_features
        self.feature = feature
        self.min_cluster_size = min_cluster_size
        self.min_samples = min_samples
        self.cluster_selection_epsilon = cluster_selection_epsilon
        self.cluster_selection_method = cluster_selection_method
        self.allow_single_cluster = allow_single_cluster
        self.prediction_data = prediction_data

        # here we need to get what we are clustering. The self.feature_info parameter will contain a number of
        # results who&#39;s dimension is dependent on the dimensions of the original data and how many features were
        # requested by the user. If it is 2D then its for a single spectrum (so this wont actually work), if its 3D then
        # its for a collection of spectra (aka a TSG dataset), if its 4D then its for an image

        # get the data dimensions and shape
        dims = self.feature_info.ndim
        original_shape = self.feature_info.shape

        # see what features are being requested
        if np.where(np.array(features_to_include) &gt; 0)[0].size &gt; 0:
            what_features = np.where(np.array(features_to_include) &gt; 0)[0]
        else:
            print(&#34;No features have been selected for clustering, so I am going to go back to sleep&#34;)
            return -99

        # if the number of features to cluster over is greater than the actual number of features extracted teh reset to
        # the maximum number of features extracted
        if number_of_features &gt; self.max_features:
            number_of_features = self.max_features

        # okay so a couple of special cases now
        # If the number_of_features is equal to 1 then we need to see what the feature to use is
        if number_of_features == 0:
            print(&#34;Yeah that&#39;s not going to work, you need at least 1 feature to work with&#34;)
            return -99

        if number_of_features == 1:
            # special case!!
            if dims == 2:
                print(&#34;Yeah, so I cant do this. Its only a single spectrum. So nothing to cluster&#34;)
                return -99
            elif dims == 3:
                data = self.feature_info[:, what_features, feature]
                data = np.reshape(data, [data.shape[0], data.shape[1]])
            elif dims == 4:
                data = self.feature_info[:, :, what_features, feature]
                data = np.reshape(data, [data.shape[0] * data.shape[1], data.shape[2]])
            else:
                print(&#34;I have no idea what this is! It has to many dimensions&#34;)
                return -99
            if data.shape[1] == 1:
                # we are effectively making a second feature that is simply the index of the data point
                linear_array = np.arange(data.shape[0])
                data = np.transpose(np.stack((linear_array, data[:, 0])))
        else:
            if dims == 2:
                print(&#34;Yeah, so I cant do this. Its only a single spectrum. So nothing to cluster&#34;)
                return -99
            elif dims == 3:
                data = self.feature_info[:, np.where(np.array(features_to_include) &gt; 0)[0], :number_of_features]
                data = np.reshape(data, [data.shape[0], data.shape[1] * data.shape[2]])
            elif dims == 4:
                data = self.feature_info[:, :, np.where(np.array(features_to_include) &gt; 0)[0], :number_of_features]
                data = np.reshape(data, [data.shape[0] * data.shape[1], data.shape[2] * data.shape[3]])
            else:
                print(&#34;I have no idea what this is! It has to many dimensions&#34;)
                return -99

        #
        scalar = StandardScaler()
        scalar.fit(data)
        self.scalar = scalar
        scaled_data = scalar.transform(data)

        # set up the HDBSCAN Class &amp; fit the data
        clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, gen_min_span_tree=True,
                                    cluster_selection_epsilon=cluster_selection_epsilon,
                                    cluster_selection_method=cluster_selection_method,
                                    allow_single_cluster=allow_single_cluster,
                                    prediction_data=prediction_data, core_dist_n_jobs=1).fit(scaled_data)

        self.clusterer = clusterer

        # Fit the data
        class_labels = clusterer.labels_
        class_probabilities = clusterer.probabilities_
        if dims == 4:
            class_labels = np.reshape(class_labels, [original_shape[0], original_shape[1]])
            class_probabilities = np.reshape(class_probabilities, [original_shape[0], original_shape[1]])

        return class_labels, class_probabilities</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="spex.extraction.specex.process_single_signal"><code class="name flex">
<span>def <span class="ident">process_single_signal</span></span>(<span>signal, ordinates, distance=None, prominence=None, max_features=4, old_school=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Get the peaks for a single spectrum</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>old_school</code></strong> :&ensp;<code>bool</code></dt>
<dd>if True use the location of the minimum and 1 channel to the left and right to use
a 2nd order polynomial to solve for the wavelength location. if False use a 6 order polynomial.</dd>
<dt><strong><code>signal</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>the signal to be processed (usually a numpy array)</dd>
<dt><strong><code>ordinates</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>the ordinates corresponding to the signal (usually a numpy array)</dd>
<dt><strong><code>distance</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Required minimal horizontal distance (&gt;= 1) in samples between neighbouring peaks.
Smaller peaks are removed first until the condition is fulfilled for all remaining peaks.</dd>
<dt><strong><code>prominence</code></strong> :&ensp;<code>number</code> or <code>ndarray</code> or <code>sequence</code>, optional</dt>
<dd>Required prominence of peaks. Either a number, <code>None</code>,
an array matching <code>x</code> or a 2-element sequence of the former. The first element is always interpreted as the
minimal and the second, if supplied, as the maximal required prominence.</dd>
<dt><strong><code>max_features</code></strong> :&ensp;<code>int</code></dt>
<dd>maximum number of features to report back</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>A numpy array of size (9 x number of requested features) values representing the feature parameters for each of
the found features</dd>
</dl>
<h2 id="comments">Comments</h2>
<p>The 9 parameters associated with a single feature are as follows,</p>
<p>0: feature wavelength</p>
<p>1: feature depth (given as prominence in the find_peaks routine). These can be considered as relative depths.
See the explanation of prominence in
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html</a></p>
<p>2: feature width (FWHM)</p>
<p>3: feature asymmetry. A number between -1 and 1. -1 is heavily left symmetrical, 0 is symmetrical and 1 is
heavily right symmetrical</p>
<p>4: feature peak heights. These are different than the feature depths. The feature peak heights are the height
from the base line to the top of the peak</p>
<p>5: Wavelength location of the left shoulder of a feature (from a prominence point of view)</p>
<p>6: Wavelength location of the right shoulder of a feature (from a prominence point of view)</p>
<p>7: Wavelength location of the left hand side of the FWHM</p>
<p>8: Wavelength location of the right hand side of the FWHM</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_single_signal(signal, ordinates, distance=None, prominence=None, max_features=4, old_school=False):
    &#34;&#34;&#34;
    Get the peaks for a single spectrum

    Args:
        old_school (bool): if True use the location of the minimum and 1 channel to the left and right to use
            a 2nd order polynomial to solve for the wavelength location. if False use a 6 order polynomial.

        signal (ndarray): the signal to be processed (usually a numpy array)

        ordinates (ndarray): the ordinates corresponding to the signal (usually a numpy array)

        distance (int, optional): Required minimal horizontal distance (&gt;= 1) in samples between neighbouring peaks.
            Smaller peaks are removed first until the condition is fulfilled for all remaining peaks.

        prominence (number or ndarray or sequence, optional): Required prominence of peaks. Either a number, ``None``,
            an array matching `x` or a 2-element sequence of the former. The first element is always interpreted as the
            minimal and the second, if supplied, as the maximal required prominence.

        max_features (int): maximum number of features to report back

    Returns:
        ndarray: A numpy array of size (9 x number of requested features) values representing the feature parameters for each of
            the found features

    Comments:
        The 9 parameters associated with a single feature are as follows,

        0: feature wavelength

        1: feature depth (given as prominence in the find_peaks routine). These can be considered as relative depths.
        See the explanation of prominence in
        https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html

        2: feature width (FWHM)

        3: feature asymmetry. A number between -1 and 1. -1 is heavily left symmetrical, 0 is symmetrical and 1 is
        heavily right symmetrical

        4: feature peak heights. These are different than the feature depths. The feature peak heights are the height
        from the base line to the top of the peak

        5: Wavelength location of the left shoulder of a feature (from a prominence point of view)

        6: Wavelength location of the right shoulder of a feature (from a prominence point of view)

        7: Wavelength location of the left hand side of the FWHM

        8: Wavelength location of the right hand side of the FWHM


    &#34;&#34;&#34;
    # # get the spectrum after taking care of the hull
    # get the indicies for the peaks. If the width isn&#39;t set to 0 (or some other value) it wont return peaks_properties
    peaks, peaks_properties = find_peaks(signal, distance=distance, prominence=prominence, width=0.0, height=0.0)
    # sort from deepest to smallest
    indices = np.flip(np.argsort(peaks_properties[&#39;prominences&#39;]))

    # get rid of additional unwanted features
    if len(indices) &gt;= max_features:
        indices = indices[:max_features]

    # reorder the data from deepest to smallest
    peaks = peaks[indices]
    for value in peaks_properties:
        peaks_properties[value] = peaks_properties[value][indices]

    # calculate the wavelength positions in the ordinate space with a 2nd order polynomial e.g quadratic
    # this is done because we assume the actual minima may be between the signal indices
    peak_ordinates = []
    if old_school:
        for t in peaks:
            if 0 &lt; t &lt; len(signal) - 2:
                x = ordinates[t - 1:t + 2]
                y = signal[t - 1:t + 2]
                coefficients = np.polyfit(x, y, 2)
                peak_ordinates.append(-coefficients[1] / (2 * coefficients[0]))
            else:
                peak_ordinates.append(ordinates[t])
    else:
        for index, t in enumerate(peaks):
            if 10 &lt; t &lt; len(signal) - 11: # end point limits
                order = 2
                if peaks_properties[&#39;widths&#39;][index] &lt; 2:
                    start = t-1
                    stop = t+2
                else:
                    start = t-10
                    stop = t + 10
                    order = 6
                newx = ordinates[start:stop]
                newy = signal[start:stop]
                coefficients, _ = rolypoly.polyfit(newx, newy, order, full=True)
                tempy = rolypoly.polyval(newx, coefficients)
                max_loc = np.argmax(tempy)
                if order &gt; 2:
                    if np.logical_or(max_loc == 0, max_loc == newx.size-1):
                        max_loc = 10
                    coefficients, _ = rolypoly.polyfit(newx[max_loc - 1:max_loc + 2], tempy[max_loc - 1:max_loc + 2], 2, full=True)
                answer = -coefficients[1] / (2 * coefficients[2])
                peak_ordinates.append(answer)
            else:
                peak_ordinates.append(ordinates[t])

    # pad out the results if needed to suit the users number of features request
    if np.size(peak_ordinates) &lt; max_features:
        peak_ordinates = np.append(peak_ordinates, np.zeros(max_features - np.size(peak_ordinates)))
        for value in peaks_properties:
            peaks_properties[value] = np.append(peaks_properties[value],
                                                np.zeros(max_features - np.size(peaks_properties[value])))

    # Interpolate the left and right locations of the FWHM to wavelength space
    left_ips = np.interp(peaks_properties[&#39;left_ips&#39;], np.arange(ordinates.shape[0]), ordinates)
    right_ips = np.interp(peaks_properties[&#39;right_ips&#39;], np.arange(ordinates.shape[0]), ordinates)
    widths = right_ips - left_ips
    left_bases = np.interp(peaks_properties[&#39;left_bases&#39;], np.arange(ordinates.shape[0]), ordinates)
    right_bases = np.interp(peaks_properties[&#39;right_bases&#39;], np.arange(ordinates.shape[0]), ordinates)
    asymmetry = np.zeros(left_ips.shape)
    indx = (right_ips - left_ips) &gt; 0
    asymmetry[indx] = 2 * ((right_ips[indx] - np.array(peak_ordinates)[indx]) / (right_ips[indx] - left_ips[indx]) - 0.5)
    return_values = np.asarray(peak_ordinates), peaks_properties[&#39;prominences&#39;], widths, asymmetry, \
                    peaks_properties[&#39;peak_heights&#39;], left_bases, right_bases, left_ips, right_ips

    return return_values</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="spex.extraction.specex.SpectralExtraction"><code class="flex name class">
<span>class <span class="ident">SpectralExtraction</span></span>
<span>(</span><span>instrument_files, max_features=4, feature_proximity=None, prominence_depth=None, ordinate_inspection_range=None, do_hull=False, hull_type=0, invert=False, order=0, nrows=None)</span>
</code></dt>
<dd>
<div class="desc"><p>A class for extracting spectral feature information from a spectral dataset. The dataset is represented by an
instance of the Instrument class e.g. from <code><a title="spex.io.instruments" href="../io/instruments.html">spex.io.instruments</a></code></p>
<p>The features that are returned (either peaks or absorptions depending on how you set it up) are not just
randomly selected between your ordinate_inspection_range they are ordered. Meaning if you said give me 4
features then it is the 4 deepest features in descending order.</p>
<p>The actual calculation of the location of the peaks is done via the find_peaks routine in scikit-learn. To get
actual peak locations in a wavelength format they are further inferred from the fitting of a 3 point
quadratic (since the band where the deepest point occurs might not technically be the actual location of the
deepest feature) and solving <code>dy/dx = 0 = -b^2/2a</code></p>
<p>find_peaks: <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html</a></p>
<p>I would recommend having a read through the various routines and returns from fit_peaks so you can get to grips
with what it actually being returned from this class.</p>
<p>It is up to the user to select the appropriate processing required in terms of hull corrections via the
<code>do_hull</code> keyword and/or invert keyword.</p>
<h2 id="warnings">Warnings</h2>
<p>YOU CAN DO SOME REALLY WEIRD STUFF IF YOU DON'T PAY ATTENTION</p>
<p>If you are chasing the spectral absorption features.If the incoming data is spectral reflectance then set
<code>do_hull=True</code> and <code>hull_type</code> to your preferred. If the data is already in the appropriate format i.e. hull removed
etc, then this isn't needed and do_hull=False should be set.</p>
<p>If the incoming spectra are TIR reflectance spectra then you set <code>do_hull=False</code> and <code>hull_type=3</code>
A bit weird but it tells the class that what you actually want is a baseline correction. If you don't want a
baseline correction then just leave hull_type as the default. As an aside, the hull type actually applied with
those settings is a hull removed type so it does not alter the heights of the peaks during the baseline
correction.</p>
<p>When you do the above with TIR spectra you are actually finding the locations of the peaks!!!
Not the reflectance absorptions. If we assume <code>1=R+E+T</code> (reflectance + emissivity + transmittance) then this is
finding the locations of the emissivity absorptions (assuming T is zero).</p>
<p>If you want to use a different hull type for TIR then do the following, set <code>invert=True</code> (does a 1-spectra),
set <code>do_hull=True</code> (this acts as a baseline correction) and set <code>hull_type</code> to your preference.</p>
<p>If you want the reflectance absorptions (emissivity peaks) of your TIR spectra then set <code>do_hull=True</code>,
<code>invert=False</code> and <code>hull_type=0</code> (hull quotient) or <code>1</code> (hull removed). There is no baseline correction here though.</p>
<p>So lots of weirdness can ensue if you don't pay attention to the keywords and your spectra.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>dimension_shape</code></strong> :&ensp;<code>tuple</code></dt>
<dd>The shape of the spectral data set</dd>
<dt><strong><code>dimensions</code></strong> :&ensp;<code>int</code></dt>
<dd>how many dimensions are in the spectral dataset</dd>
<dt><strong><code>do_hull</code></strong> :&ensp;<code>bool</code></dt>
<dd>using a hull correction prior to feature extraction</dd>
<dt><strong><code>feature_info</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>the extracted feature information</dd>
<dt><strong><code>feature_proximity</code></strong> :&ensp;<code>int</code></dt>
<dd>closest allowable feature distance</dd>
<dt><strong><code>feature_search_indices</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>A boolean array of the same sample size as the spectral data where the
feature_search_space criteria was met.</dd>
<dt><strong><code>feature_search_parameters</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>The result of running a feature search over the extracted spectral features</dd>
<dt><strong><code>feature_search_space</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of tuples designating the search criteria of the feature_info</dd>
<dt><strong><code>hull_type</code></strong></dt>
<dd>The hull type used in the feature extraction</dd>
<dt><strong><code>instrument</code></strong> :&ensp;<code>obj</code></dt>
<dd>An instance of the <code><a title="spex.io.instruments" href="../io/instruments.html">spex.io.instruments</a></code> class</dd>
<dt><strong><code>invert</code></strong> :&ensp;<code>bool</code></dt>
<dd>Should the spectral data be inverted prior to feature extraction</dd>
<dt><strong><code>max_features</code></strong> :&ensp;<code>int</code></dt>
<dd>How many spectral features to find</dd>
<dt><strong><code>ordinates</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>the ordinates of the spectral data</dd>
<dt><strong><code>ordinates_inspection_range</code></strong> :&ensp;<code>list</code></dt>
<dd>The spectral range over which to perform feature extraction</dd>
<dt><strong><code>prominence_depth</code></strong> :&ensp;<code>float</code></dt>
<dd>minimum acceptable prominence depth</dd>
<dt><strong><code>range_index</code></strong> :&ensp;<code>list</code></dt>
<dd>the indices of the ordinates array that correspond to the ordinates_inspection_range</dd>
</dl>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>instrument_files</code></strong> :&ensp;<code>list</code></dt>
<dd>A list that depending on the contents will be used to open the spectral data. The
can have the following form [csv path+filename], or [wavelengths(ndarray), spectra (ndarray)], or
[envi spectral path+filename, envi spectral hdr path+filename], or [tsg spectral path+filename, tsg *.tsg path+filename]</dd>
<dt><strong><code>max_features</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The maximum number of features to report back. Defaults to 4</dd>
<dt><strong><code>feature_proximity</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The closest distance between features to be considered as a spectral
feature. Defaults to None. This is an index value so a value of 3 says I wont accept any absorptions that are
closer than 3 bands to the left and right of my feature (dominant feature always wins)</dd>
<dt><strong><code>prominence_depth</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The minimum feature depth to be considered. Defaults to None</dd>
<dt><strong><code>ordinate_inspection_range</code></strong> :&ensp;<code>list</code>, optional</dt>
<dd>A start and stop bounding range for the feature search. Defaults to
the instruments spectral range e.g. [start, stop] -&gt; [2000., 2500.]</dd>
<dt><strong><code>do_hull</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Perform hull operations on the incoming spectral data. Defaults to False</dd>
<dt><strong><code>hull_type</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>0: hull quotient, 1: hull removed 3: baseline correction, Defaults to 0</dd>
<dt><strong><code>invert</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>should the spectra be inverted (e.g. 1-spectra) before doing any processing, Defaults to False</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SpectralExtraction:
    &#34;&#34;&#34;
    A class for extracting spectral feature information from a spectral dataset. The dataset is represented by an
    instance of the Instrument class e.g. from `spex.io.instruments`

    The features that are returned (either peaks or absorptions depending on how you set it up) are not just
    randomly selected between your ordinate_inspection_range they are ordered. Meaning if you said give me 4
    features then it is the 4 deepest features in descending order.

    The actual calculation of the location of the peaks is done via the find_peaks routine in scikit-learn. To get
    actual peak locations in a wavelength format they are further inferred from the fitting of a 3 point
    quadratic (since the band where the deepest point occurs might not technically be the actual location of the
    deepest feature) and solving ``dy/dx = 0 = -b^2/2a``

    find_peaks: https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html

    I would recommend having a read through the various routines and returns from fit_peaks so you can get to grips
    with what it actually being returned from this class.

    It is up to the user to select the appropriate processing required in terms of hull corrections via the
    ``do_hull`` keyword and/or invert keyword.

    Warnings:
        YOU CAN DO SOME REALLY WEIRD STUFF IF YOU DON&#39;T PAY ATTENTION

        If you are chasing the spectral absorption features.If the incoming data is spectral reflectance then set
        ``do_hull=True`` and ``hull_type`` to your preferred. If the data is already in the appropriate format i.e. hull removed
        etc, then this isn&#39;t needed and do_hull=False should be set.

        If the incoming spectra are TIR reflectance spectra then you set ``do_hull=False`` and ``hull_type=3``
        A bit weird but it tells the class that what you actually want is a baseline correction. If you don&#39;t want a
        baseline correction then just leave hull_type as the default. As an aside, the hull type actually applied with
        those settings is a hull removed type so it does not alter the heights of the peaks during the baseline
        correction.

        When you do the above with TIR spectra you are actually finding the locations of the peaks!!!
        Not the reflectance absorptions. If we assume ``1=R+E+T`` (reflectance + emissivity + transmittance) then this is
        finding the locations of the emissivity absorptions (assuming T is zero).

    If you want to use a different hull type for TIR then do the following, set ``invert=True`` (does a 1-spectra),
    set ``do_hull=True`` (this acts as a baseline correction) and set ``hull_type`` to your preference.

    If you want the reflectance absorptions (emissivity peaks) of your TIR spectra then set ``do_hull=True``,
    ``invert=False`` and ``hull_type=0`` (hull quotient) or ``1`` (hull removed). There is no baseline correction here though.

    So lots of weirdness can ensue if you don&#39;t pay attention to the keywords and your spectra.

    Attributes:
        dimension_shape (tuple): The shape of the spectral data set

        dimensions (int): how many dimensions are in the spectral dataset

        do_hull (bool): using a hull correction prior to feature extraction

        feature_info (ndarray): the extracted feature information

        feature_proximity (int): closest allowable feature distance

        feature_search_indices (ndarray): A boolean array of the same sample size as the spectral data where the
            feature_search_space criteria was met.

        feature_search_parameters (ndarray): The result of running a feature search over the extracted spectral features

        feature_search_space (list): A list of tuples designating the search criteria of the feature_info

        hull_type: The hull type used in the feature extraction

        instrument (obj): An instance of the `spex.io.instruments` class

        invert (bool): Should the spectral data be inverted prior to feature extraction

        max_features (int): How many spectral features to find

        ordinates (ndarray): the ordinates of the spectral data

        ordinates_inspection_range (list): The spectral range over which to perform feature extraction

        prominence_depth (float): minimum acceptable prominence depth

        range_index (list): the indices of the ordinates array that correspond to the ordinates_inspection_range

    &#34;&#34;&#34;

    def __init__(self, instrument_files, max_features=4, feature_proximity=None, prominence_depth=None,
                 ordinate_inspection_range=None, do_hull=False, hull_type=0, invert=False, order=0, nrows=None):
        &#34;&#34;&#34;
        Args:
            instrument_files (list): A list that depending on the contents will be used to open the spectral data. The
                can have the following form [csv path+filename], or [wavelengths(ndarray), spectra (ndarray)], or
                [envi spectral path+filename, envi spectral hdr path+filename], or [tsg spectral path+filename, tsg *.tsg path+filename]

            max_features (int, optional): The maximum number of features to report back. Defaults to 4

            feature_proximity (int, optional): The closest distance between features to be considered as a spectral
                feature. Defaults to None. This is an index value so a value of 3 says I wont accept any absorptions that are
                closer than 3 bands to the left and right of my feature (dominant feature always wins)

            prominence_depth (float, optional): The minimum feature depth to be considered. Defaults to None

            ordinate_inspection_range (list, optional): A start and stop bounding range for the feature search. Defaults to
                the instruments spectral range e.g. [start, stop] -&gt; [2000., 2500.]

            do_hull (bool, optional): Perform hull operations on the incoming spectral data. Defaults to False

            hull_type (int, optional): 0: hull quotient, 1: hull removed 3: baseline correction, Defaults to 0

            invert (bool, optional): should the spectra be inverted (e.g. 1-spectra) before doing any processing, Defaults to False

        &#34;&#34;&#34;
        if isinstance(instrument_files, Tsg):
            self.instrument = instrument_files
        else:
            io_length = len(instrument_files)
            is_numpy = isinstance(instrument_files[0], np.ndarray)
            if io_length &gt; 1:
                envi_or_not = instrument_files[1].__contains__(&#34;.hdr&#34;)
                if is_numpy:
                    self.instrument = NumpySpectra(instrument_files)
                else:
                    if envi_or_not:
                        self.instrument = ImageData(instrument_files)
                    else:
                        self.instrument = Tsg(instrument_files)
            else:
                self.instrument = CsvSpectra(instrument_files, order=order, nrows=nrows)

        self.dimensions = self.instrument.dimensions
        self.dimension_shape = self.instrument.dimension_shape
        self.do_hull = do_hull
        self.hull_type = hull_type
        self.invert = invert
        self.ordinates = self.instrument.get_ordinates()
        # self.metadata = instrument.metadata
        self._generator = self.instrument.datagenerator()

        if ordinate_inspection_range is None:
            self.ordinates_inspection_range = [self.ordinates[0], self.ordinates[-1]]
            self.range_index = _find_indices([self.ordinates[0], self.ordinates[-1]], self.ordinates)
        else:
            self.ordinates_inspection_range = ordinate_inspection_range
            self.range_index = _find_indices(ordinate_inspection_range, self.ordinates)

        # feature extraction parameters
        self.max_features = max_features
        self.feature_proximity = feature_proximity
        self.prominence_depth = prominence_depth

        # the extracted feature info populates this variable
        self.feature_info = None
        # self.average_feature_info = None

        # feature searching data
        self.feature_search_indices = None
        self.feature_search_space = None
        self.feature_search_parameters = None

        # clustering parameters
        self.scalar = None
        self.clusterer = None
        self.features_to_include = [1, 0, 0, 0, 0, 0, 0, 0, 0]
        self.number_of_features = 2
        self.feature = 0
        self.min_cluster_size = 5
        self.min_samples = None
        self.cluster_selection_epsilon = 0.0
        self.cluster_selection_method = &#39;eom&#39;
        self.allow_single_cluster = False
        self.prediction_data = False

    def process_data(self):
        &#34;&#34;&#34;
        Process the spectral data according to the class instantiation variables and return the feature results

        Returns:
            An array of values for each found feature (all zeros if no feature is found). The array has dimensions
            of (N x 9 x number of features requested), where is N are the number of spectral samples. The features are given
            from the largest to the smallest so entry features[0,:,0] is for the first feature, feature[0,:,1] is for the
            second and so on.

        Comments:
            The 9 parameters associated with a single feature are as follows:

            0: feature wavelength

            1: feature depth (given as prominence in the find_peaks routine). These can be considered as relative depths.
            See the explanation of prominence in
            https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html

            2: feature width (FWHM)

            3: feature asymmetry. A number between -1 and 1. -1 is heavily left symmetrical, 0 is symmetrical and 1 is
            heavily right symmetrical

            4: feature peak heights. These are different than the feature depths. The feature peak heights are the height
            from the base line to the top of the peak

            5: Wavelength location of the left shoulder of a feature (from a prominence point of view)

            6: Wavelength location of the right shoulder of a feature (from a prominence point of view)

            7: Wavelength location of the left hand side of the FWHM

            8: Wavelength location of the right hand side of the FWHM

        &#34;&#34;&#34;
        self.feature_info = np.squeeze(self._process_spectral_array())
        return self.feature_info

    def get_features(self):
        &#34;&#34;&#34;
        Return the extracted feature information

        Returns:
            An array of values for each found feature (all zeros if no feature is found). The array has dimensions
            of (N x 9 x number of features requested) or (N x M x 9 x number of features requested), where the first is
            for N samples and the second is for an NxM image. The features are given from the largest to the smallest
            so that entry features[0,:,0] is for the first feature, feature[0,:,1] is for the
            second and so on.

        Comments:
            The 9 parameters associated with a single feature are as follows:

            0: feature wavelength

            1: feature depth (given as prominence in the find_peaks routine). These can be considered as relative depths.
            See the explanation of prominence in
            https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html

            2: feature width (FWHM)

            3: feature asymmetry. A number between -1 and 1. -1 is heavily left symmetrical, 0 is symmetrical and 1 is
            heavily right symmetrical

            4: feature peak heights. These are different than the feature depths. The feature peak heights are the height
            from the base line to the top of the peak

            5: Wavelength location of the left shoulder of a feature (from a prominence point of view)

            6: Wavelength location of the right shoulder of a feature (from a prominence point of view)

            7: Wavelength location of the left hand side of the FWHM

            8: Wavelength location of the right hand side of the FWHM

        &#34;&#34;&#34;
        return self.feature_info

    def _feature_search(self, search_space, exclude=False):
        &#34;&#34;&#34;
        Search N feature_info entries in the provided search space

        Args:
            search_space (list): A list comprising the search space

        Returns:
            same as feature_search

        &#34;&#34;&#34;
        feature_info = self.feature_info

        s_space = np.array(search_space)
        starts = s_space[::2]
        stops = s_space[1::2]
        diffs = stops - starts
        search = np.ones(feature_info[..., 0, :].shape).astype(bool)
        for index, val in enumerate(diffs):
            if val &gt; 0:
                temp = (feature_info[..., index, :] &gt; starts[index]) &amp; (feature_info[..., index, :] &lt; stops[index])
                search = search &amp; temp

        if exclude:
            search = np.max(search, axis=-1)

        return search

    @staticmethod
    def find_first_occurence(x):
        y = np.zeros_like(x, dtype=bool)
        idx = np.arange(len(x)), x.argmax(axis=1)
        y[idx] = x[idx]
        return y

    @staticmethod
    def static_feature_search(search_space, exclude=False, feature_info=None, single=False):
        &#34;&#34;&#34;
        Search N feature_info entries in the provided search space

        Args:
            search_space (list): A list comprising the search space

        Returns:
            same as feature_search

        &#34;&#34;&#34;
        if feature_info is None:
            return -99

        s_space = np.array(search_space)
        starts = s_space[::2]
        stops = s_space[1::2]
        diffs = stops - starts
        search = np.ones(feature_info[..., 0, :].shape).astype(bool)
        for index, val in enumerate(diffs):
            if val &gt; 0:
                temp = (feature_info[..., index, :] &gt; starts[index]) &amp; (feature_info[..., index, :] &lt; stops[index])
                search = search &amp; temp

        if exclude:
            search = np.max(search, axis=-1)
        if single:
            search = SpectralExtraction.find_first_occurence(search)

        return search

    def _single_spectrum_search(self, search_space):
        &#34;&#34;&#34;
        Search the single spectrum feature_info

        Args:
            search_space (list): list of tuples comprising the search space

        Returns:
            A boolean array of size 1 indicating if it satisfied the search criteria, an array of feature parameters
            corresponding to those samples that meet the search criteria.
            -99 if nothing found

        &#34;&#34;&#34;
        feature_info = self.feature_info
        combos_keep = []
        # (wavelength, wavelength radius, depth minimum, width, width radius, use asymmetry, asymmetry gt or lt 0)
        for combos in search_space:
            if len(combos) == 7:
                tester = np.isclose(feature_info[0, :], np.abs(combos[0]), atol=combos[1])
                if combos[0] &lt; 0:
                    tester = np.logical_not(tester)
                else:
                    # see if the user cares about the wavelength
                    if combos[0] != 0:
                        wave_test = np.isclose(feature_info[0, :], np.abs(combos[0]), atol=combos[1])
                    else:
                        # here we are saying that the wavelength is not of interest
                        wave_test = True
                    depth_test = feature_info[1, :] &gt; combos[2]
                    tester = np.logical_and(tester, depth_test)
                    if combos[3] &gt; 0:
                        widths = np.isclose(feature_info[2, :], combos[3], atol=combos[4])
                        tester = np.logical_and(tester, widths)
                    if combos[5] != 0:
                        if combos[5] &lt; 0:
                            asymmetry = feature_info[3, :] &lt; combos[6]
                        else:
                            asymmetry = feature_info[3, :] &gt; combos[6]
                        tester = np.logical_and(tester, asymmetry)
                    tester = np.logical_and(tester, wave_test)

                # append this combination to the results
                combos_keep.append(self._first_appearance_only(tester))
                # the first_appearance_only is so we don&#39;t include multiple results from one sample e.g. a deepest
                # wavelength and a 10th deepest
        else:
            print(
                &#39;The feature space combinations are not of length 7 (wavelength, wavelength radius, depth minimum, &#39;
                &#39;width, width radius, asymmetry left or right, lt or gt) &#39;)

        # keep only those entries for which all combo searches are true
        all_true = np.all(np.sum(np.asarray(combos_keep)).astype(bool))

        # return an boolean array with the same spatial size as the input spectral array
        self.feature_search_indices = all_true

        combo_feature_parameters = []
        for index, var in enumerate(combos_keep):
            if search_space[index][0] &gt;= 0:
                what_feature = var
                combo_feature_parameters.append(feature_info[:, what_feature])
                self.feature_search_parameters = np.transpose(np.asarray(combo_feature_parameters), (1, 0))

        if self.feature_search_parameters.size == 0:
            self.feature_search_parameters = -99
            self.feature_search_indices = -99

        return self.feature_search_indices, self.feature_search_parameters

    def _profile_spectrum_search(self, search_space):
        &#34;&#34;&#34;
        Search N feature_info entries in the provided search space

        Args:
            search_space (list): list of tuples comprising the search space

        Returns:
            same as feature_search

        &#34;&#34;&#34;
        feature_info = self.feature_info
        combos_keep = []

        # # build the search
        #         # q = (feature_info[:, 1, :] &gt; 0.2) &amp; (feature_info[:, 0, :] &gt; 800) &amp; (feature_info[:, 0, :] &lt; 1000) &amp; (
        #         #             feature_info[:, 2, :] &gt; 100) &amp; (feature_info[:, 2, :] &lt; 400)
        #         # # identify which samples matched the search
        #         # qq = np.where(np.max(
        #         #     (feature_info[:, 1, :] &gt; 0.2) &amp; (feature_info[:, 0, :] &gt; 800) &amp; (feature_info[:, 0, :] &lt; 1000) &amp; (
        #         #                 feature_info[:, 2, :] &gt; 100) &amp; (feature_info[:, 2, :] &lt; 400), axis=1))[0]
        #         # # get the feature that matched the search
        #         # feat = np.argmax(q[qq, :], axis=1)
        # #  get the actual features
        # bob = feature_info[qq, :, feat]
        # (wavelength, wavelength radius, depth minimum, width, width radius)
        # a = (feature_info[:, 0, :] &gt; 750) &amp; (feature_info[:, 0, :] &lt; 1000)
        # b = (feature_info[:, 1, :] &gt; 0.0) &amp; (feature_info[:, 1, :] &lt; 0.5)
        # c = (feature_info[:, 2, :] &gt; 100) &amp; (feature_info[:, 2, :] &lt; 400)
        # d = (feature_info[:, 3, :] &gt; 0) &amp; (feature_info[:, 3, :] &lt; 1)
        # m = np.max(a &amp; b &amp; c &amp; d, axis=-1)
        # am = np.argmax(a &amp; b &amp; c &amp; d, axis=-1)

        for combos in search_space:
            if len(combos) == 7:
                if combos[0] &lt; 0:
                    wave_test = np.isclose(feature_info[:, 0, :], np.abs(combos[0]), atol=combos[1])
                    null_samples = np.where(wave_test)
                    wave_test[null_samples[0], :] = True
                    wave_test = np.logical_not(wave_test)
                    combos_keep.append(wave_test)
                else:
                    # see if the user cares about the wavelength
                    if combos[0] != 0:
                        wave_test = np.isclose(feature_info[:, 0, :], np.abs(combos[0]), atol=combos[1])
                    else:
                        # here we are saying that the wavelength is not of interest
                        wave_test = np.ones((feature_info.shape[0], feature_info.shape[2]), dtype=bool)
                    depth_test = feature_info[:, 1, :] &gt;= combos[2]
                    if combos[3] &gt; 0:
                        widths = np.isclose(feature_info[:, 2, :], combos[3], atol=combos[4])
                        tester = np.logical_and(depth_test, widths)
                    else:
                        tester = depth_test
                    if combos[5] != 0:
                        if combos[5] &lt; 0:
                            asymmetry = feature_info[:, 3, :] &lt; combos[6]
                        else:
                            asymmetry = feature_info[:, 3, :] &gt; combos[6]
                        tester = np.logical_and(tester, asymmetry)
                    tester = np.logical_and(tester, wave_test)

                    # append this combination to the results
                    combos_keep.append(self._first_appearance_only(tester))
                    # the first_appearance_only is so we don&#39;t include multiple results from one sample e.g. a deepest
                    # wavelength and a 10th deepest
            else:
                print(
                    &#39;The feature space combinations are not of length 5 (wavelength, wavelength radius, &#39;
                    &#39;depth minimum, width, width radius) &#39;)

        # keep only those entries for which all combo searches are true
        all_true = np.all(np.sum(np.asarray(combos_keep), axis=2).astype(bool), axis=0)

        # return an boolean array with the same spatial size as the input spectral array
        self.feature_search_indices = all_true

        # okay this works (this is for a 2D spatial image e.g. corescan)
        all_true_indices = np.where(all_true)
        combo_feature_parameters = []
        for index, var in enumerate(combos_keep):
            if search_space[index][0] &gt;= 0:
                what_feature = np.where(var[all_true_indices[0], :])[1]
                combo_feature_parameters.append(feature_info[all_true_indices[0], :, what_feature])
                self.feature_search_parameters = np.transpose(np.asarray(combo_feature_parameters), (1, 2, 0))

        if self.feature_search_parameters.size == 0:
            self.feature_search_parameters = -99
        if self.feature_search_indices.size == 0:
            self.feature_search_indices = -99

        return self.feature_search_indices, self.feature_search_parameters

    def _imager_spectral_search(self, search_space):
        &#34;&#34;&#34;
        Search the image returned feature_info in the provided search space

        Args:
            search_space (list): list of tuples comprising the search space

        Returns:
            same as feature_Search

        &#34;&#34;&#34;

        feature_info = self.feature_info
        combos_keep = []
        # (wavelength, wavelength radius, depth minimum, width, width radius)
        for combos in search_space:
            if len(combos) == 7:
                if combos[0] &lt; 0:
                    wave_test = np.isclose(feature_info[:, :, 0, :], np.abs(combos[0]), atol=combos[1])
                    null_samples = np.where(wave_test)
                    wave_test[null_samples[0], null_samples[1], :] = True
                    wave_test = np.logical_not(wave_test)
                    combos_keep.append(wave_test)
                else:
                    # see if the user cares about the wavelength
                    if combos[0] != 0:
                        wave_test = np.isclose(feature_info[:, :, 0, :], np.abs(combos[0]), atol=combos[1])
                    else:
                        # here we are saying that the wavelength is not of interest
                        wave_test = np.ones((feature_info.shape[0], feature_info.shape[1], feature_info.shape[3]),
                                            dtype=bool)
                    depth_test = feature_info[:, :, 1, :] &gt;= combos[2]
                    if combos[3] &gt; 0:
                        widths = np.isclose(feature_info[:, :, 2, :], combos[3], atol=combos[4])
                        tester = np.logical_and(depth_test, widths)
                    else:
                        tester = depth_test
                    if combos[5] != 0:
                        if combos[5] &lt; 0:
                            asymmetry = feature_info[:, :, 3, :] &lt; combos[6]
                        else:
                            asymmetry = feature_info[:, :, 3, :] &gt; combos[6]
                        tester = np.logical_and(tester, asymmetry)

                    tester = np.logical_and(tester, wave_test)
                    # append this combination to the results
                    combos_keep.append(self._first_appearance_only(tester))
                    # the first_appearance_only is so we dont inlcude multiple results from one sample e.g. a deepest
                    # wavelength and a 10th deepest
            else:
                print(
                    &#39;The feature space combinations are not of length 7 (wavelength, wavelength radius, &#39;
                    &#39;depth minimum, width, width radius) &#39;)

        # keep only those entries for which all combo searches are true
        all_true = np.all(np.sum(np.asarray(combos_keep), axis=3).astype(bool), axis=0)

        # return an boolean array with the same spatial size as the input spectral array
        self.feature_search_indices = all_true

        # okay this works (this is for a 2D spatial image e.g. corescan)
        all_true_indices = np.where(all_true)
        combo_feature_parameters = []
        for index, var in enumerate(combos_keep):
            if search_space[index][0] &gt;= 0:
                what_feature = np.where(var[all_true_indices[0], all_true_indices[1], :])[1]
                combo_feature_parameters.append(
                    feature_info[all_true_indices[0], all_true_indices[1], :, what_feature])
            self.feature_search_parameters = np.transpose(np.asarray(combo_feature_parameters), (1, 2, 0))

        if self.feature_search_parameters.size == 0:
            self.feature_search_parameters = -99
        if self.feature_search_indices.size == 0:
            self.feature_search_indices = -99

        return self.feature_search_indices, self.feature_search_parameters

    def feature_search(self, search_space=[0, 0, 0, 0, 0, 0, 0, 0], exclude=False):
        &#34;&#34;&#34;
        Return a boolean array of locations where the search space criteria is satisfied and an array of parameters
        that correspond to the feature information for those samples who meet the search criteria

        Args:
            search_space (list): A list of tuples that define the search space parameters [(search 1), (search 2),...(search N)]

        A single_tuple consists of: (wavelength, wavelength search radius, minimum depth, width, width search radius,
         asymmetry direction, asymmetry &lt; or &gt;)

        Returns:
            feature_search_indices, feature_search_parameters (or -99 if nothing found)

             feature_search_indices (ndarray): A boolean array of the same sample size as the spectral data where the
                feature_search_space criteria was met.

            feature_search_parameters (ndarray): The result of running a feature search over the extracted spectral features

        Comments:
            In a given search tuple a value of 0 for the wavelength, or width generally  means I don&#39;t care about this
            parameter. So if you had a search tuple that looked like this (0,0,0.1,0,0,0,0) you would be saying I am
            only interested in all results where the minimum depth is greater than 0.1

            If you want to do a search and exclude a wavelength i.e. find all results where returned features do NOT
            contain this wavelength then set the wavelength to a negative value e.g. (-2210,..)
            NOTE: You should put your wavelength exclusions at the end of the search space list. The reason why is that
            even though a search space may have an exclusion in it the return is only comprised of the positive search&#39;s
            (see below for an example). So if you stick an exclusion in the middle of your entire search space, which
            you can do if you want, you need to keep track yourself of what search space the return values pertain to.

            If you want to search on asymmetry then set the 6th search parameter to a number greater than 0 for right
            symmetric or less than 0 for left symmetric. Set the last parameter to find those values either greater than
            or less than (dependant on left or right symmetric)

            So for example a search_space of,
            [(2160, 10, 0, 0, 0, 0, 0), (2210, 20, 0, 30, 5, 0, 0), (-2250, 20, 0, 0, 0, 0, 0)]
            will look for all samples where a feature is found to have feature wavelengths between (2160+/-10) AND
            (2210+/-20 with feature widths of 30+/-5) AND (no 2250+/-20 feature).
            The feature information returned though is only for the positive search components.

            If we use the example above and assume the features were initially extracted from an array
            of 5000 samples of which 120 matched the search criteria then it would return the following:

            boolean array: (5000), A numpy array: (120, 9, 2) NOTE: Only 2 search space criteria returned
            Any wavelengths set to negative (as in exclude those) are not included in the return result.

            Referencing the second array returned at (:,0,0) would give the wavelengths found for the first
            search_space (2160+/-10), (:,1,0) would give the depths and (:,2,0) would give the width and so on.
            (:,0,1) would give the wavelengths found for the second search_space (2210+/-20), (:,1,1)
             would give the depths, and (:,2,1) would give the widths (30+/-5) and so on.

        &#34;&#34;&#34;
        if self.feature_info is None:
            print(&#39;You need to process the data first: .process_data()&#39;)
            return

        ndims = self.dimensions + 1
        if len(search_space) == 8:
            return self._feature_search(search_space, exclude=exclude)

        # # todo get rid of the range check. if folks want to search outside of their range then let them.
        # # see if any of the search space values are outside of the spectral feature extraction ordinate inspection range
        # # if they are then they will produce a False simply by not being within the feature extraction inspection range
        # # To avoid this we clip the search range. It might not be the best approach but...
        # temp_search_space = []
        # for val in search_space:
        #     if abs(val[0]) &gt; 0:
        #         test_search = np.logical_and(abs(val[0]) &gt;= self.ordinates_inspection_range[0],
        #                                      abs(val[0]) &lt;= self.ordinates_inspection_range[1])
        #         if test_search:
        #             temp_search_space.append(val)
        #     else:
        #         temp_search_space.append(val)
        #
        # self.feature_search_space = search_space
        #
        # # (wavelength start, wavelength stop, depth start, depth stop, width start, width stop,
        # # asymetry start, asymetry stop)
        # for combos in search_space:
        #     if len(combos) == 7:
        #         if ndims == 2:
        #             return self._single_spectrum_search(search_space)
        #         elif ndims == 3:
        #             return self._profile_spectrum_search(search_space)
        #         elif ndims == 4:
        #             return self._imager_spectral_search(search_space)
        #         else:
        #             return -99, -99
        #     else:
        #         print(
        #             &#39;The feature space combinations are not of length 7 (wavelength, wavelength radius, &#39;
        #             &#39;depth minimum, width, width radius) &#39;)
        #         return -99

    @staticmethod
    def _first_appearance_only(x):
        &#34;&#34;&#34;
        Sets all occurrences of True in the last dimension to False with the exception of the first occurrence. Its used
        to ensure we don&#39;t keep duplicate values. In hindsight this can probably be done with np.unique I suspect or
        something like that. I&#39;ll look into that at a later date

        Args:
            x (): A numpy array of type bool eith 1D, 2D or 3D

        Returns:
            A numpy array of type bool

        &#34;&#34;&#34;
        y = np.zeros_like(x, dtype=bool)
        if x.ndim == 1:
            y = x.argmax()
        if x.ndim == 2:
            idx = np.arange(len(x)), x.argmax(axis=1)
            y[idx] = x[idx]
        elif x.ndim == 3:
            for index, var in enumerate(x):
                idx = np.arange(len(var)), var.argmax(axis=1)
                y[index, idx[0], idx[1]] = var[idx[0], idx[1]]
        return y

    def feature_data_to_new_range(self, new_range):
        &#34;&#34;&#34;
        Trims the returned feature info down to a new spectral range and returns it
        Does not overwrite the original data but it also does not store it.

        Args:
            new_range (list): a list of start and stop wavelength e.g. [2100, 2400]

        Returns:
            the extracted spectral feature information between the new_range

        &#34;&#34;&#34;
        max_number = 0
        indices = []
        if self.feature_info is None:
            self.process_data()

        # new_features_array = np.zeros(self.feature_info.shape)
        #
        # if self.dimensions == 2:
        #     pass
        # elif self.dimensions == 3:
        #     locations = np.where(np.logical_and(self.feature_info[:, :, 0, :] &gt;= new_range[0],
        #                             self.feature_info[:, :, 0, :] &lt;= new_range[1]))
        #     how_big = locations[0].size
        #     if how_big &gt; 0:
        #         new_features_array[locations[0], locations[1], :, :] = \
        #             self.feature_info[locations[0], locations[1], :, :]

        feature_data = self.feature_info
        for val in np.arange(feature_data.shape[0]):
            locations = np.where(np.logical_and(feature_data[val, 0, :] &gt;= new_range[0],
                                                feature_data[val, 0, :] &lt;= new_range[1]))[0]
            how_big = locations.size

            if how_big &gt; 0:
                indices.append((val, locations))
            if how_big &gt; max_number:
                max_number = how_big

        new_features_array = np.zeros((feature_data.shape[0], feature_data.shape[1], max_number))
        for val in indices:
            new_features_array[val[0], :, :val[1].size] = feature_data[val[0], :, :][:, val[1]]

        if new_features_array.size == 0:
            new_features_array = -99

        return new_features_array

    def get_feature_parameters(self):
        &#34;&#34;&#34;
        returns the results of a feature_search

        Returns:
            feature_search_indices, feature_search_parameters (or -99 if nothing found)

            feature_search_indices (ndarray): A boolean array of the same sample size as the spectral data where the
                feature_search_space criteria was met.

            feature_search_parameters (ndarray): The result of running a feature search over the extracted spectral features

        &#34;&#34;&#34;
        return self.feature_search_indices, self.feature_search_parameters

    def _process_spectral_array(self):
        &#34;&#34;&#34;
        This is an internal method so we can work with spectral image data.

        Returns:
            feature_info[N, M, 9] where N is the number of rows and M is the number of columns

        Comments:
            The 9 parameters associated with a single feature are as follows:

            0: feature wavelength

            1: feature depth (given as prominence in the find_peaks routine). These can be considered as relative depths.
            See the explanation of prominence in
            https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html

            2: feature width (FWHM)

            3:Feature asymmetry. A number between -1 and 1. -1 is heavily left symmetrical, 0 is symmetrical and 1 is
            heavily right symmetrical

            4: feature peak heights. These are different than the feature depths. The feature peak heights are the height
            from the base line to the top of the peak

            5: Wavelength location of the left shoulder of a feature (from a prominence point of view)

            6: Wavelength location of the right shoulder of a feature (from a prominence point of view)

            7: Wavelength location of the left hand side of the FWHM

            8: Wavelength location of the right hand side of the FWHM

        &#34;&#34;&#34;
        ordinate_range = self.ordinates_inspection_range
        ordinates = self.ordinates
        do_hull = self.do_hull
        hull_type = self.hull_type
        invert = self.invert
        feature_proximity = self.feature_proximity
        max_features = self.max_features
        prominence = self.prominence_depth
        spectral_array = self.instrument.return_all_spectra()
        ndims = self.dimensions

        feature_info = []
        # subset the data on wavelength if its called for
        if ordinate_range is not None:
            range_index = _find_indices(ordinate_range, ordinates)
            ordinates = ordinates[range_index[0]:range_index[1] + 1]
            if ndims == 1:
                spectral_array = spectral_array[range_index[0]:range_index[1] + 1]
            elif ndims == 2:
                spectral_array = spectral_array[:, range_index[0]:range_index[1] + 1]
            elif ndims == 3:
                spectral_array = spectral_array[:, :, range_index[0]:range_index[1] + 1]
            else:
                return -99
        spectral_array[spectral_array &lt; 0] = 0

        if invert:
            spectral_array = 1.0 - spectral_array

        # run a hull process if required
        if do_hull:
            spectral_array = get_absorption(ordinates, spectral_array, hull_type=hull_type)
        if hull_type == 3:
            # do a baseline correction. Should really only do this for spectral data
            spectral_array = get_absorption(ordinates, 1.0 - spectral_array, hull_type=1)

        if ndims != 3:
            for index, signal in enumerate(_generator(spectral_array)):
                # return the peaks_ordinates, prominences and widths
                feature_info.append(
                    process_single_signal(signal, ordinates, distance=feature_proximity, max_features=max_features,
                                          prominence=prominence))
        else:  # multiprocessing
            # for value in spectral_array:
            #     for signal in _generator(value):
            #         # return the peaks_ordinates, prominences and widths
            #         feature_info.append(
            #             process_single_signal(signal, ordinates, distance=feature_proximity, max_features=max_features,
            #                                   prominence=prominence))
            # The bit above is not how i wanted to do this but for some reasons unknown to me the multiprocessing
            # has now decided to be a massive pain in the arse and stop working. Ah well long processing times for
            # images it is then
            # Nope found the problem. To use multiprocessing on windows the routine has to be in a __main__ guard

            chunks = mp.cpu_count()
            rows = int(spectral_array.shape[0] / chunks)
            sub_arrays = np.split(spectral_array, [rows * val for val in np.arange(1, chunks)])
            inputs = []
            for entry in sub_arrays:
                inputs.append((entry, ordinates, self.feature_proximity, self.max_features, self.prominence_depth))
            pool = mp.Pool(chunks)
            pool_output = pool.starmap(_mp_process_data, inputs)  # Returns a list of lists
            pool.close()
            pool.join()
            feature_info = [entry for val in pool_output for entry in val]

        # store the data
        if spectral_array.ndim == 3:
            feature_info = np.reshape(np.asarray(feature_info),
                                      (spectral_array.shape[0], spectral_array.shape[1], 9, max_features))
        else:
            feature_info = np.asarray(feature_info)

        return feature_info

    def cluster_features(self, features_to_include=[1, 0, 0, 0, 0, 0, 0, 0, 0], number_of_features=2, feature=0,
                         min_cluster_size=5, min_samples=None, cluster_selection_epsilon=0.0,
                         cluster_selection_method=&#39;eom&#39;, allow_single_cluster=False, prediction_data=False):
        &#34;&#34;&#34;
        A method for clustering of the extracted features  as returned by `SpectralExtraction.process_data`.

        Additional:
            https://hdbscan.readthedocs.io/en/latest/index.html

            https://towardsdatascience.com/a-gentle-introduction-to-hdbscan-and-density-based-clustering-5fd79329c1e8

            https://pberba.github.io/stats/2020/01/17/hdbscan/

        Args:
            feature (int): which feature to use for clustering. This only applies if `number_of_features=1`

            prediction_data (bool): Whether to generate extra cached data for predicting labels or
                membership vectors few new unseen points later. If you wish to
                persist the clustering object for later re-use you probably want
                to set this to True. Defaults to False

            number_of_features (int): How many features to use for the clustering. Goes from deepest to smallest e.g. if
                `number_of_features = 2` then it will use the 1st and 2nd deepest features.

            features_to_include (list): A list of length 9 consisting of 1&#39;s and 0&#39;s used to switch a given feature on or
                off

            min_cluster_size (int): The minimum size of clusters; single linkage splits that contain
                fewer points than this will be considered points &#34;falling out&#34; of a
                cluster rather than a cluster splitting into two new clusters.

            min_samples (int): The number of samples in a neighbourhood for a point to be considered a core point.

            cluster_selection_epsilon (float): A distance threshold. Clusters below this value will be merged.

            cluster_selection_method (str): The method used to select clusters from the condensed tree. The
                standard approach for HDBSCAN* is to use an Excess of Mass algorithm
                to find the most persistent clusters. Alternatively you can instead
                select the clusters at the leaves of the tree -- this provides the
                most fine grained and homogeneous clusters. Options are: ``eom`` or ``leaf``

            allow_single_cluster (bool): By default HDBSCAN* will not produce a single cluster, setting this
                to True will override this and allow single cluster results in
                the case that you feel this is a valid result for your dataset.

        Returns:
            -99: when the clustering can not be completed (a message is printed)

            class_labels (ndarry): An array of the same dimensions (non spectral) as the input data containing the
                resultant class labels as returned by `HDBSCAN`. The input data are the extracted spectral features.

            class_probabilities (ndarray): An array of the same dimensions (non spectral) as the input data containing
                the cluster probabilities as returned by `HDBSCAN`. The input data are the extracted spectral features.

        &#34;&#34;&#34;
        # first check if any feature info exists
        if self.feature_info is None:
            print(&#34;You need to do a feature extraction first using process_data()&#34;)
            self.process_data()

        # write some information to the state variables
        self.features_to_include = features_to_include
        self.number_of_features = number_of_features
        self.feature = feature
        self.min_cluster_size = min_cluster_size
        self.min_samples = min_samples
        self.cluster_selection_epsilon = cluster_selection_epsilon
        self.cluster_selection_method = cluster_selection_method
        self.allow_single_cluster = allow_single_cluster
        self.prediction_data = prediction_data

        # here we need to get what we are clustering. The self.feature_info parameter will contain a number of
        # results who&#39;s dimension is dependent on the dimensions of the original data and how many features were
        # requested by the user. If it is 2D then its for a single spectrum (so this wont actually work), if its 3D then
        # its for a collection of spectra (aka a TSG dataset), if its 4D then its for an image

        # get the data dimensions and shape
        dims = self.feature_info.ndim
        original_shape = self.feature_info.shape

        # see what features are being requested
        if np.where(np.array(features_to_include) &gt; 0)[0].size &gt; 0:
            what_features = np.where(np.array(features_to_include) &gt; 0)[0]
        else:
            print(&#34;No features have been selected for clustering, so I am going to go back to sleep&#34;)
            return -99

        # if the number of features to cluster over is greater than the actual number of features extracted teh reset to
        # the maximum number of features extracted
        if number_of_features &gt; self.max_features:
            number_of_features = self.max_features

        # okay so a couple of special cases now
        # If the number_of_features is equal to 1 then we need to see what the feature to use is
        if number_of_features == 0:
            print(&#34;Yeah that&#39;s not going to work, you need at least 1 feature to work with&#34;)
            return -99

        if number_of_features == 1:
            # special case!!
            if dims == 2:
                print(&#34;Yeah, so I cant do this. Its only a single spectrum. So nothing to cluster&#34;)
                return -99
            elif dims == 3:
                data = self.feature_info[:, what_features, feature]
                data = np.reshape(data, [data.shape[0], data.shape[1]])
            elif dims == 4:
                data = self.feature_info[:, :, what_features, feature]
                data = np.reshape(data, [data.shape[0] * data.shape[1], data.shape[2]])
            else:
                print(&#34;I have no idea what this is! It has to many dimensions&#34;)
                return -99
            if data.shape[1] == 1:
                # we are effectively making a second feature that is simply the index of the data point
                linear_array = np.arange(data.shape[0])
                data = np.transpose(np.stack((linear_array, data[:, 0])))
        else:
            if dims == 2:
                print(&#34;Yeah, so I cant do this. Its only a single spectrum. So nothing to cluster&#34;)
                return -99
            elif dims == 3:
                data = self.feature_info[:, np.where(np.array(features_to_include) &gt; 0)[0], :number_of_features]
                data = np.reshape(data, [data.shape[0], data.shape[1] * data.shape[2]])
            elif dims == 4:
                data = self.feature_info[:, :, np.where(np.array(features_to_include) &gt; 0)[0], :number_of_features]
                data = np.reshape(data, [data.shape[0] * data.shape[1], data.shape[2] * data.shape[3]])
            else:
                print(&#34;I have no idea what this is! It has to many dimensions&#34;)
                return -99

        #
        scalar = StandardScaler()
        scalar.fit(data)
        self.scalar = scalar
        scaled_data = scalar.transform(data)

        # set up the HDBSCAN Class &amp; fit the data
        clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, gen_min_span_tree=True,
                                    cluster_selection_epsilon=cluster_selection_epsilon,
                                    cluster_selection_method=cluster_selection_method,
                                    allow_single_cluster=allow_single_cluster,
                                    prediction_data=prediction_data, core_dist_n_jobs=1).fit(scaled_data)

        self.clusterer = clusterer

        # Fit the data
        class_labels = clusterer.labels_
        class_probabilities = clusterer.probabilities_
        if dims == 4:
            class_labels = np.reshape(class_labels, [original_shape[0], original_shape[1]])
            class_probabilities = np.reshape(class_probabilities, [original_shape[0], original_shape[1]])

        return class_labels, class_probabilities</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="spex.extraction.specex.SpectralExtraction.find_first_occurence"><code class="name flex">
<span>def <span class="ident">find_first_occurence</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def find_first_occurence(x):
    y = np.zeros_like(x, dtype=bool)
    idx = np.arange(len(x)), x.argmax(axis=1)
    y[idx] = x[idx]
    return y</code></pre>
</details>
</dd>
<dt id="spex.extraction.specex.SpectralExtraction.static_feature_search"><code class="name flex">
<span>def <span class="ident">static_feature_search</span></span>(<span>search_space, exclude=False, feature_info=None, single=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Search N feature_info entries in the provided search space</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>search_space</code></strong> :&ensp;<code>list</code></dt>
<dd>A list comprising the search space</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>same as feature_search</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def static_feature_search(search_space, exclude=False, feature_info=None, single=False):
    &#34;&#34;&#34;
    Search N feature_info entries in the provided search space

    Args:
        search_space (list): A list comprising the search space

    Returns:
        same as feature_search

    &#34;&#34;&#34;
    if feature_info is None:
        return -99

    s_space = np.array(search_space)
    starts = s_space[::2]
    stops = s_space[1::2]
    diffs = stops - starts
    search = np.ones(feature_info[..., 0, :].shape).astype(bool)
    for index, val in enumerate(diffs):
        if val &gt; 0:
            temp = (feature_info[..., index, :] &gt; starts[index]) &amp; (feature_info[..., index, :] &lt; stops[index])
            search = search &amp; temp

    if exclude:
        search = np.max(search, axis=-1)
    if single:
        search = SpectralExtraction.find_first_occurence(search)

    return search</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="spex.extraction.specex.SpectralExtraction.cluster_features"><code class="name flex">
<span>def <span class="ident">cluster_features</span></span>(<span>self, features_to_include=[1, 0, 0, 0, 0, 0, 0, 0, 0], number_of_features=2, feature=0, min_cluster_size=5, min_samples=None, cluster_selection_epsilon=0.0, cluster_selection_method='eom', allow_single_cluster=False, prediction_data=False)</span>
</code></dt>
<dd>
<div class="desc"><p>A method for clustering of the extracted features
as returned by <code><a title="spex.extraction.specex.SpectralExtraction.process_data" href="#spex.extraction.specex.SpectralExtraction.process_data">SpectralExtraction.process_data()</a></code>.</p>
<h2 id="additional">Additional</h2>
<p><a href="https://hdbscan.readthedocs.io/en/latest/index.html">https://hdbscan.readthedocs.io/en/latest/index.html</a></p>
<p><a href="https://towardsdatascience.com/a-gentle-introduction-to-hdbscan-and-density-based-clustering-5fd79329c1e8">https://towardsdatascience.com/a-gentle-introduction-to-hdbscan-and-density-based-clustering-5fd79329c1e8</a></p>
<p><a href="https://pberba.github.io/stats/2020/01/17/hdbscan/">https://pberba.github.io/stats/2020/01/17/hdbscan/</a></p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>feature</code></strong> :&ensp;<code>int</code></dt>
<dd>which feature to use for clustering. This only applies if <code>number_of_features=1</code></dd>
<dt><strong><code>prediction_data</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to generate extra cached data for predicting labels or
membership vectors few new unseen points later. If you wish to
persist the clustering object for later re-use you probably want
to set this to True. Defaults to False</dd>
<dt><strong><code>number_of_features</code></strong> :&ensp;<code>int</code></dt>
<dd>How many features to use for the clustering. Goes from deepest to smallest e.g. if
<code>number_of_features = 2</code> then it will use the 1st and 2nd deepest features.</dd>
<dt><strong><code>features_to_include</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of length 9 consisting of 1's and 0's used to switch a given feature on or
off</dd>
<dt><strong><code>min_cluster_size</code></strong> :&ensp;<code>int</code></dt>
<dd>The minimum size of clusters; single linkage splits that contain
fewer points than this will be considered points "falling out" of a
cluster rather than a cluster splitting into two new clusters.</dd>
<dt><strong><code>min_samples</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of samples in a neighbourhood for a point to be considered a core point.</dd>
<dt><strong><code>cluster_selection_epsilon</code></strong> :&ensp;<code>float</code></dt>
<dd>A distance threshold. Clusters below this value will be merged.</dd>
<dt><strong><code>cluster_selection_method</code></strong> :&ensp;<code>str</code></dt>
<dd>The method used to select clusters from the condensed tree. The
standard approach for HDBSCAN* is to use an Excess of Mass algorithm
to find the most persistent clusters. Alternatively you can instead
select the clusters at the leaves of the tree &ndash; this provides the
most fine grained and homogeneous clusters. Options are: <code>eom</code> or <code>leaf</code></dd>
<dt><strong><code>allow_single_cluster</code></strong> :&ensp;<code>bool</code></dt>
<dd>By default HDBSCAN* will not produce a single cluster, setting this
to True will override this and allow single cluster results in
the case that you feel this is a valid result for your dataset.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>-99: when the clustering can not be completed (a message is printed)</p>
<dl>
<dt><code>class_labels (ndarry): An array</code> of <code>the same dimensions (non spectral) as the input data containing the</code></dt>
<dd>resultant class labels as returned by <code>HDBSCAN</code>. The input data are the extracted spectral features.</dd>
<dt><code>class_probabilities (ndarray): An array</code> of <code>the same dimensions (non spectral) as the input data containing</code></dt>
<dd>the cluster probabilities as returned by <code>HDBSCAN</code>. The input data are the extracted spectral features.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cluster_features(self, features_to_include=[1, 0, 0, 0, 0, 0, 0, 0, 0], number_of_features=2, feature=0,
                     min_cluster_size=5, min_samples=None, cluster_selection_epsilon=0.0,
                     cluster_selection_method=&#39;eom&#39;, allow_single_cluster=False, prediction_data=False):
    &#34;&#34;&#34;
    A method for clustering of the extracted features  as returned by `SpectralExtraction.process_data`.

    Additional:
        https://hdbscan.readthedocs.io/en/latest/index.html

        https://towardsdatascience.com/a-gentle-introduction-to-hdbscan-and-density-based-clustering-5fd79329c1e8

        https://pberba.github.io/stats/2020/01/17/hdbscan/

    Args:
        feature (int): which feature to use for clustering. This only applies if `number_of_features=1`

        prediction_data (bool): Whether to generate extra cached data for predicting labels or
            membership vectors few new unseen points later. If you wish to
            persist the clustering object for later re-use you probably want
            to set this to True. Defaults to False

        number_of_features (int): How many features to use for the clustering. Goes from deepest to smallest e.g. if
            `number_of_features = 2` then it will use the 1st and 2nd deepest features.

        features_to_include (list): A list of length 9 consisting of 1&#39;s and 0&#39;s used to switch a given feature on or
            off

        min_cluster_size (int): The minimum size of clusters; single linkage splits that contain
            fewer points than this will be considered points &#34;falling out&#34; of a
            cluster rather than a cluster splitting into two new clusters.

        min_samples (int): The number of samples in a neighbourhood for a point to be considered a core point.

        cluster_selection_epsilon (float): A distance threshold. Clusters below this value will be merged.

        cluster_selection_method (str): The method used to select clusters from the condensed tree. The
            standard approach for HDBSCAN* is to use an Excess of Mass algorithm
            to find the most persistent clusters. Alternatively you can instead
            select the clusters at the leaves of the tree -- this provides the
            most fine grained and homogeneous clusters. Options are: ``eom`` or ``leaf``

        allow_single_cluster (bool): By default HDBSCAN* will not produce a single cluster, setting this
            to True will override this and allow single cluster results in
            the case that you feel this is a valid result for your dataset.

    Returns:
        -99: when the clustering can not be completed (a message is printed)

        class_labels (ndarry): An array of the same dimensions (non spectral) as the input data containing the
            resultant class labels as returned by `HDBSCAN`. The input data are the extracted spectral features.

        class_probabilities (ndarray): An array of the same dimensions (non spectral) as the input data containing
            the cluster probabilities as returned by `HDBSCAN`. The input data are the extracted spectral features.

    &#34;&#34;&#34;
    # first check if any feature info exists
    if self.feature_info is None:
        print(&#34;You need to do a feature extraction first using process_data()&#34;)
        self.process_data()

    # write some information to the state variables
    self.features_to_include = features_to_include
    self.number_of_features = number_of_features
    self.feature = feature
    self.min_cluster_size = min_cluster_size
    self.min_samples = min_samples
    self.cluster_selection_epsilon = cluster_selection_epsilon
    self.cluster_selection_method = cluster_selection_method
    self.allow_single_cluster = allow_single_cluster
    self.prediction_data = prediction_data

    # here we need to get what we are clustering. The self.feature_info parameter will contain a number of
    # results who&#39;s dimension is dependent on the dimensions of the original data and how many features were
    # requested by the user. If it is 2D then its for a single spectrum (so this wont actually work), if its 3D then
    # its for a collection of spectra (aka a TSG dataset), if its 4D then its for an image

    # get the data dimensions and shape
    dims = self.feature_info.ndim
    original_shape = self.feature_info.shape

    # see what features are being requested
    if np.where(np.array(features_to_include) &gt; 0)[0].size &gt; 0:
        what_features = np.where(np.array(features_to_include) &gt; 0)[0]
    else:
        print(&#34;No features have been selected for clustering, so I am going to go back to sleep&#34;)
        return -99

    # if the number of features to cluster over is greater than the actual number of features extracted teh reset to
    # the maximum number of features extracted
    if number_of_features &gt; self.max_features:
        number_of_features = self.max_features

    # okay so a couple of special cases now
    # If the number_of_features is equal to 1 then we need to see what the feature to use is
    if number_of_features == 0:
        print(&#34;Yeah that&#39;s not going to work, you need at least 1 feature to work with&#34;)
        return -99

    if number_of_features == 1:
        # special case!!
        if dims == 2:
            print(&#34;Yeah, so I cant do this. Its only a single spectrum. So nothing to cluster&#34;)
            return -99
        elif dims == 3:
            data = self.feature_info[:, what_features, feature]
            data = np.reshape(data, [data.shape[0], data.shape[1]])
        elif dims == 4:
            data = self.feature_info[:, :, what_features, feature]
            data = np.reshape(data, [data.shape[0] * data.shape[1], data.shape[2]])
        else:
            print(&#34;I have no idea what this is! It has to many dimensions&#34;)
            return -99
        if data.shape[1] == 1:
            # we are effectively making a second feature that is simply the index of the data point
            linear_array = np.arange(data.shape[0])
            data = np.transpose(np.stack((linear_array, data[:, 0])))
    else:
        if dims == 2:
            print(&#34;Yeah, so I cant do this. Its only a single spectrum. So nothing to cluster&#34;)
            return -99
        elif dims == 3:
            data = self.feature_info[:, np.where(np.array(features_to_include) &gt; 0)[0], :number_of_features]
            data = np.reshape(data, [data.shape[0], data.shape[1] * data.shape[2]])
        elif dims == 4:
            data = self.feature_info[:, :, np.where(np.array(features_to_include) &gt; 0)[0], :number_of_features]
            data = np.reshape(data, [data.shape[0] * data.shape[1], data.shape[2] * data.shape[3]])
        else:
            print(&#34;I have no idea what this is! It has to many dimensions&#34;)
            return -99

    #
    scalar = StandardScaler()
    scalar.fit(data)
    self.scalar = scalar
    scaled_data = scalar.transform(data)

    # set up the HDBSCAN Class &amp; fit the data
    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples, gen_min_span_tree=True,
                                cluster_selection_epsilon=cluster_selection_epsilon,
                                cluster_selection_method=cluster_selection_method,
                                allow_single_cluster=allow_single_cluster,
                                prediction_data=prediction_data, core_dist_n_jobs=1).fit(scaled_data)

    self.clusterer = clusterer

    # Fit the data
    class_labels = clusterer.labels_
    class_probabilities = clusterer.probabilities_
    if dims == 4:
        class_labels = np.reshape(class_labels, [original_shape[0], original_shape[1]])
        class_probabilities = np.reshape(class_probabilities, [original_shape[0], original_shape[1]])

    return class_labels, class_probabilities</code></pre>
</details>
</dd>
<dt id="spex.extraction.specex.SpectralExtraction.feature_data_to_new_range"><code class="name flex">
<span>def <span class="ident">feature_data_to_new_range</span></span>(<span>self, new_range)</span>
</code></dt>
<dd>
<div class="desc"><p>Trims the returned feature info down to a new spectral range and returns it
Does not overwrite the original data but it also does not store it.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>new_range</code></strong> :&ensp;<code>list</code></dt>
<dd>a list of start and stop wavelength e.g. [2100, 2400]</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>the extracted spectral feature information between the new_range</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def feature_data_to_new_range(self, new_range):
    &#34;&#34;&#34;
    Trims the returned feature info down to a new spectral range and returns it
    Does not overwrite the original data but it also does not store it.

    Args:
        new_range (list): a list of start and stop wavelength e.g. [2100, 2400]

    Returns:
        the extracted spectral feature information between the new_range

    &#34;&#34;&#34;
    max_number = 0
    indices = []
    if self.feature_info is None:
        self.process_data()

    # new_features_array = np.zeros(self.feature_info.shape)
    #
    # if self.dimensions == 2:
    #     pass
    # elif self.dimensions == 3:
    #     locations = np.where(np.logical_and(self.feature_info[:, :, 0, :] &gt;= new_range[0],
    #                             self.feature_info[:, :, 0, :] &lt;= new_range[1]))
    #     how_big = locations[0].size
    #     if how_big &gt; 0:
    #         new_features_array[locations[0], locations[1], :, :] = \
    #             self.feature_info[locations[0], locations[1], :, :]

    feature_data = self.feature_info
    for val in np.arange(feature_data.shape[0]):
        locations = np.where(np.logical_and(feature_data[val, 0, :] &gt;= new_range[0],
                                            feature_data[val, 0, :] &lt;= new_range[1]))[0]
        how_big = locations.size

        if how_big &gt; 0:
            indices.append((val, locations))
        if how_big &gt; max_number:
            max_number = how_big

    new_features_array = np.zeros((feature_data.shape[0], feature_data.shape[1], max_number))
    for val in indices:
        new_features_array[val[0], :, :val[1].size] = feature_data[val[0], :, :][:, val[1]]

    if new_features_array.size == 0:
        new_features_array = -99

    return new_features_array</code></pre>
</details>
</dd>
<dt id="spex.extraction.specex.SpectralExtraction.feature_search"><code class="name flex">
<span>def <span class="ident">feature_search</span></span>(<span>self, search_space=[0, 0, 0, 0, 0, 0, 0, 0], exclude=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Return a boolean array of locations where the search space criteria is satisfied and an array of parameters
that correspond to the feature information for those samples who meet the search criteria</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>search_space</code></strong> :&ensp;<code>list</code></dt>
<dd>A list of tuples that define the search space parameters [(search 1), (search 2),&hellip;(search N)]</dd>
</dl>
<p>A single_tuple consists of: (wavelength, wavelength search radius, minimum depth, width, width search radius,
asymmetry direction, asymmetry &lt; or &gt;)</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>feature_search_indices, feature_search_parameters (or -99 if nothing found)</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>feature_search_indices (ndarray): A boolean array of the same sample size as the spectral data where the
feature_search_space criteria was met.</p>
<dl>
<dt><code>feature_search_parameters (ndarray): The result</code> of <code>running a feature search over the extracted spectral features</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="comments">Comments</h2>
<p>In a given search tuple a value of 0 for the wavelength, or width generally
means I don't care about this
parameter. So if you had a search tuple that looked like this (0,0,0.1,0,0,0,0) you would be saying I am
only interested in all results where the minimum depth is greater than 0.1</p>
<p>If you want to do a search and exclude a wavelength i.e. find all results where returned features do NOT
contain this wavelength then set the wavelength to a negative value e.g. (-2210,..)
NOTE: You should put your wavelength exclusions at the end of the search space list. The reason why is that
even though a search space may have an exclusion in it the return is only comprised of the positive search's
(see below for an example). So if you stick an exclusion in the middle of your entire search space, which
you can do if you want, you need to keep track yourself of what search space the return values pertain to.</p>
<p>If you want to search on asymmetry then set the 6th search parameter to a number greater than 0 for right
symmetric or less than 0 for left symmetric. Set the last parameter to find those values either greater than
or less than (dependant on left or right symmetric)</p>
<p>So for example a search_space of,
[(2160, 10, 0, 0, 0, 0, 0), (2210, 20, 0, 30, 5, 0, 0), (-2250, 20, 0, 0, 0, 0, 0)]
will look for all samples where a feature is found to have feature wavelengths between (2160+/-10) AND
(2210+/-20 with feature widths of 30+/-5) AND (no 2250+/-20 feature).
The feature information returned though is only for the positive search components.</p>
<p>If we use the example above and assume the features were initially extracted from an array
of 5000 samples of which 120 matched the search criteria then it would return the following:</p>
<p>boolean array: (5000), A numpy array: (120, 9, 2) NOTE: Only 2 search space criteria returned
Any wavelengths set to negative (as in exclude those) are not included in the return result.</p>
<p>Referencing the second array returned at (:,0,0) would give the wavelengths found for the first
search_space (2160+/-10), (:,1,0) would give the depths and (:,2,0) would give the width and so on.
(:,0,1) would give the wavelengths found for the second search_space (2210+/-20), (:,1,1)
would give the depths, and (:,2,1) would give the widths (30+/-5) and so on.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def feature_search(self, search_space=[0, 0, 0, 0, 0, 0, 0, 0], exclude=False):
    &#34;&#34;&#34;
    Return a boolean array of locations where the search space criteria is satisfied and an array of parameters
    that correspond to the feature information for those samples who meet the search criteria

    Args:
        search_space (list): A list of tuples that define the search space parameters [(search 1), (search 2),...(search N)]

    A single_tuple consists of: (wavelength, wavelength search radius, minimum depth, width, width search radius,
     asymmetry direction, asymmetry &lt; or &gt;)

    Returns:
        feature_search_indices, feature_search_parameters (or -99 if nothing found)

         feature_search_indices (ndarray): A boolean array of the same sample size as the spectral data where the
            feature_search_space criteria was met.

        feature_search_parameters (ndarray): The result of running a feature search over the extracted spectral features

    Comments:
        In a given search tuple a value of 0 for the wavelength, or width generally  means I don&#39;t care about this
        parameter. So if you had a search tuple that looked like this (0,0,0.1,0,0,0,0) you would be saying I am
        only interested in all results where the minimum depth is greater than 0.1

        If you want to do a search and exclude a wavelength i.e. find all results where returned features do NOT
        contain this wavelength then set the wavelength to a negative value e.g. (-2210,..)
        NOTE: You should put your wavelength exclusions at the end of the search space list. The reason why is that
        even though a search space may have an exclusion in it the return is only comprised of the positive search&#39;s
        (see below for an example). So if you stick an exclusion in the middle of your entire search space, which
        you can do if you want, you need to keep track yourself of what search space the return values pertain to.

        If you want to search on asymmetry then set the 6th search parameter to a number greater than 0 for right
        symmetric or less than 0 for left symmetric. Set the last parameter to find those values either greater than
        or less than (dependant on left or right symmetric)

        So for example a search_space of,
        [(2160, 10, 0, 0, 0, 0, 0), (2210, 20, 0, 30, 5, 0, 0), (-2250, 20, 0, 0, 0, 0, 0)]
        will look for all samples where a feature is found to have feature wavelengths between (2160+/-10) AND
        (2210+/-20 with feature widths of 30+/-5) AND (no 2250+/-20 feature).
        The feature information returned though is only for the positive search components.

        If we use the example above and assume the features were initially extracted from an array
        of 5000 samples of which 120 matched the search criteria then it would return the following:

        boolean array: (5000), A numpy array: (120, 9, 2) NOTE: Only 2 search space criteria returned
        Any wavelengths set to negative (as in exclude those) are not included in the return result.

        Referencing the second array returned at (:,0,0) would give the wavelengths found for the first
        search_space (2160+/-10), (:,1,0) would give the depths and (:,2,0) would give the width and so on.
        (:,0,1) would give the wavelengths found for the second search_space (2210+/-20), (:,1,1)
         would give the depths, and (:,2,1) would give the widths (30+/-5) and so on.

    &#34;&#34;&#34;
    if self.feature_info is None:
        print(&#39;You need to process the data first: .process_data()&#39;)
        return

    ndims = self.dimensions + 1
    if len(search_space) == 8:
        return self._feature_search(search_space, exclude=exclude)</code></pre>
</details>
</dd>
<dt id="spex.extraction.specex.SpectralExtraction.get_feature_parameters"><code class="name flex">
<span>def <span class="ident">get_feature_parameters</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>returns the results of a feature_search</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>feature_search_indices, feature_search_parameters (or -99 if nothing found)</code></dt>
<dd>&nbsp;</dd>
<dt><code>feature_search_indices (ndarray): A boolean array</code> of <code>the same sample size as the spectral data where the</code></dt>
<dd>feature_search_space criteria was met.</dd>
<dt><code>feature_search_parameters (ndarray): The result</code> of <code>running a feature search over the extracted spectral features</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_feature_parameters(self):
    &#34;&#34;&#34;
    returns the results of a feature_search

    Returns:
        feature_search_indices, feature_search_parameters (or -99 if nothing found)

        feature_search_indices (ndarray): A boolean array of the same sample size as the spectral data where the
            feature_search_space criteria was met.

        feature_search_parameters (ndarray): The result of running a feature search over the extracted spectral features

    &#34;&#34;&#34;
    return self.feature_search_indices, self.feature_search_parameters</code></pre>
</details>
</dd>
<dt id="spex.extraction.specex.SpectralExtraction.get_features"><code class="name flex">
<span>def <span class="ident">get_features</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Return the extracted feature information</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>An array</code> of <code>values for each found feature (all zeros if no feature is found). The array has dimensions</code></dt>
<dd>&nbsp;</dd>
<dt><code>of (N x 9 x number</code> of <code>features requested)</code> or <code>(N x M x 9 x number</code> of <code>features requested), where the first is</code></dt>
<dd>&nbsp;</dd>
<dt><code>for N samples and the second is for an NxM image. The features are given from the largest to the smallest</code></dt>
<dd>&nbsp;</dd>
<dt><code>so that entry features[0,:,0] is for the first feature, feature[0,:,1] is for the</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>second and so on.</p>
<h2 id="comments">Comments</h2>
<p>The 9 parameters associated with a single feature are as follows:</p>
<p>0: feature wavelength</p>
<p>1: feature depth (given as prominence in the find_peaks routine). These can be considered as relative depths.
See the explanation of prominence in
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html</a></p>
<p>2: feature width (FWHM)</p>
<p>3: feature asymmetry. A number between -1 and 1. -1 is heavily left symmetrical, 0 is symmetrical and 1 is
heavily right symmetrical</p>
<p>4: feature peak heights. These are different than the feature depths. The feature peak heights are the height
from the base line to the top of the peak</p>
<p>5: Wavelength location of the left shoulder of a feature (from a prominence point of view)</p>
<p>6: Wavelength location of the right shoulder of a feature (from a prominence point of view)</p>
<p>7: Wavelength location of the left hand side of the FWHM</p>
<p>8: Wavelength location of the right hand side of the FWHM</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_features(self):
    &#34;&#34;&#34;
    Return the extracted feature information

    Returns:
        An array of values for each found feature (all zeros if no feature is found). The array has dimensions
        of (N x 9 x number of features requested) or (N x M x 9 x number of features requested), where the first is
        for N samples and the second is for an NxM image. The features are given from the largest to the smallest
        so that entry features[0,:,0] is for the first feature, feature[0,:,1] is for the
        second and so on.

    Comments:
        The 9 parameters associated with a single feature are as follows:

        0: feature wavelength

        1: feature depth (given as prominence in the find_peaks routine). These can be considered as relative depths.
        See the explanation of prominence in
        https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html

        2: feature width (FWHM)

        3: feature asymmetry. A number between -1 and 1. -1 is heavily left symmetrical, 0 is symmetrical and 1 is
        heavily right symmetrical

        4: feature peak heights. These are different than the feature depths. The feature peak heights are the height
        from the base line to the top of the peak

        5: Wavelength location of the left shoulder of a feature (from a prominence point of view)

        6: Wavelength location of the right shoulder of a feature (from a prominence point of view)

        7: Wavelength location of the left hand side of the FWHM

        8: Wavelength location of the right hand side of the FWHM

    &#34;&#34;&#34;
    return self.feature_info</code></pre>
</details>
</dd>
<dt id="spex.extraction.specex.SpectralExtraction.process_data"><code class="name flex">
<span>def <span class="ident">process_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Process the spectral data according to the class instantiation variables and return the feature results</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>An array</code> of <code>values for each found feature (all zeros if no feature is found). The array has dimensions</code></dt>
<dd>&nbsp;</dd>
<dt><code>of (N x 9 x number</code> of <code>features requested), where is N are the number</code> of <code>spectral samples. The features are given</code></dt>
<dd>&nbsp;</dd>
<dt><code>from the largest to the smallest so entry features[0,:,0] is for the first feature, feature[0,:,1] is for the</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>second and so on.</p>
<h2 id="comments">Comments</h2>
<p>The 9 parameters associated with a single feature are as follows:</p>
<p>0: feature wavelength</p>
<p>1: feature depth (given as prominence in the find_peaks routine). These can be considered as relative depths.
See the explanation of prominence in
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html</a></p>
<p>2: feature width (FWHM)</p>
<p>3: feature asymmetry. A number between -1 and 1. -1 is heavily left symmetrical, 0 is symmetrical and 1 is
heavily right symmetrical</p>
<p>4: feature peak heights. These are different than the feature depths. The feature peak heights are the height
from the base line to the top of the peak</p>
<p>5: Wavelength location of the left shoulder of a feature (from a prominence point of view)</p>
<p>6: Wavelength location of the right shoulder of a feature (from a prominence point of view)</p>
<p>7: Wavelength location of the left hand side of the FWHM</p>
<p>8: Wavelength location of the right hand side of the FWHM</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_data(self):
    &#34;&#34;&#34;
    Process the spectral data according to the class instantiation variables and return the feature results

    Returns:
        An array of values for each found feature (all zeros if no feature is found). The array has dimensions
        of (N x 9 x number of features requested), where is N are the number of spectral samples. The features are given
        from the largest to the smallest so entry features[0,:,0] is for the first feature, feature[0,:,1] is for the
        second and so on.

    Comments:
        The 9 parameters associated with a single feature are as follows:

        0: feature wavelength

        1: feature depth (given as prominence in the find_peaks routine). These can be considered as relative depths.
        See the explanation of prominence in
        https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.find_peaks.html

        2: feature width (FWHM)

        3: feature asymmetry. A number between -1 and 1. -1 is heavily left symmetrical, 0 is symmetrical and 1 is
        heavily right symmetrical

        4: feature peak heights. These are different than the feature depths. The feature peak heights are the height
        from the base line to the top of the peak

        5: Wavelength location of the left shoulder of a feature (from a prominence point of view)

        6: Wavelength location of the right shoulder of a feature (from a prominence point of view)

        7: Wavelength location of the left hand side of the FWHM

        8: Wavelength location of the right hand side of the FWHM

    &#34;&#34;&#34;
    self.feature_info = np.squeeze(self._process_spectral_array())
    return self.feature_info</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="spex.extraction" href="index.html">spex.extraction</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="spex.extraction.specex.process_single_signal" href="#spex.extraction.specex.process_single_signal">process_single_signal</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="spex.extraction.specex.SpectralExtraction" href="#spex.extraction.specex.SpectralExtraction">SpectralExtraction</a></code></h4>
<ul class="">
<li><code><a title="spex.extraction.specex.SpectralExtraction.cluster_features" href="#spex.extraction.specex.SpectralExtraction.cluster_features">cluster_features</a></code></li>
<li><code><a title="spex.extraction.specex.SpectralExtraction.feature_data_to_new_range" href="#spex.extraction.specex.SpectralExtraction.feature_data_to_new_range">feature_data_to_new_range</a></code></li>
<li><code><a title="spex.extraction.specex.SpectralExtraction.feature_search" href="#spex.extraction.specex.SpectralExtraction.feature_search">feature_search</a></code></li>
<li><code><a title="spex.extraction.specex.SpectralExtraction.find_first_occurence" href="#spex.extraction.specex.SpectralExtraction.find_first_occurence">find_first_occurence</a></code></li>
<li><code><a title="spex.extraction.specex.SpectralExtraction.get_feature_parameters" href="#spex.extraction.specex.SpectralExtraction.get_feature_parameters">get_feature_parameters</a></code></li>
<li><code><a title="spex.extraction.specex.SpectralExtraction.get_features" href="#spex.extraction.specex.SpectralExtraction.get_features">get_features</a></code></li>
<li><code><a title="spex.extraction.specex.SpectralExtraction.process_data" href="#spex.extraction.specex.SpectralExtraction.process_data">process_data</a></code></li>
<li><code><a title="spex.extraction.specex.SpectralExtraction.static_feature_search" href="#spex.extraction.specex.SpectralExtraction.static_feature_search">static_feature_search</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.1</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>